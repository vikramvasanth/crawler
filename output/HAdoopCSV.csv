Minimum Required Skills:
Java, C++, C++, E-Commerce, Server Side Web Development, Web Services, AWS, 
Hadoop, MapReduce

If you are a Sr. server side software engineer with an interest in 
Hadoop/Pig/MapReduce please read on!

Generous full relocation assistance provided!
We are seeking a Back end Java heavy software manager to lead a very high profile development group within one if not the premier internet brand on the planet.
We offer a peerless compensation package along with technical or management career paths that have no ceiling on growth.

What You Will Be Doing

-Be a member of the back end data input development group for one of the highest profile web brands on the planet. Imagine a company on par with Amazon, Google, Apple or Facebook!
-Working in a small highly talented group of server side Java engineers.
-Build a bullet proof backend for a highly visible, very high transaction web presence with one of the most respected online brands on the planet!
-Build highly complex web analytic tools that will be used daily by millions of customers.

What You Need for this Position

-5 plus years developing some of the very best back end big data web services .
-Server Side Java 
-Ruby or Python experience
-5+ years of experience with Big Data ingestion technologies 
-Web Services experience (AWS, SaaS, EC2, etc.)
-
Hadoop, Elastic, Map Reduce, Spark

What's In It for You

-Ability to work for an internet brand that is considered peerless in their niche
-Opportunity to work in downtown San Francisco BART and Cal Train close.
-Opportunity to build your skills and resume that will provide career path security for decades to come.
-Work in highly creative team using agile methods and practices.
-Opportunity to work with cutting edge service oriented architectures and web services.So, if you are a software manager with the acumen and desire to work for an online brand on par with Google, Facebook, Amazon or Apple please apply today!

Applicants must be authorized to work in the U.S.

Please apply directly to by clicking 'Click Here to Apply' with your Word resume!

Looking forward to receiving your resume and going over the position in more detail with you.

- Not a fit for this position? Click the link at the bottom of this email to search all of our open positions.

Looking forward to receiving your resume!

CyberCoders

CyberCoders, Inc is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work - In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

Copyright &copy; 1999 - 2014. CyberCoders, Inc. All rights reserved.



 Responsibilities:


 &middot;&nbsp;&nbsp;Administrate a 
 hadoop cluster. This includes maintaining the cluster, running analysis jobs, and handling researcher support requests involving highly sensitive data, helping them formulate efficient database queries.


 &nbsp;


 Qualifications:


 &middot;&nbsp;&nbsp;Expert in Cloudera stack infrastructure (HBase, Impala, and Hive, etc)


 &nbsp;


 Rush resumes to 
 nanditha.pattabhi@akshaya-inc.com OR call me at (925) 307 7250



Minimum Required Skills:
Expert Linux Systems Administration, Working in enormous - agile enterprise environment, Amazon AWS (EC2, ELB, S3, Glacier), Support Java/J2EE-based apps running on Tomcat, Bash/Perl scripting, Strong SQL (ability to query and join tables), Git/SVN, Puppet/Chef, Have managed Cisco or Juniper, cobbler/kickstart

*5-10+years working in an enterprise environment as a Systems Engineer (Site Reliability). 



Keep reading if you're passionate about your craft and you're looking to contribute at a high level. We are a major player in our space as a massive, data (via JavaScript) aggregating, enterprise Java/Big Data software company. We are providing amazing solutions for all kinds of companies in in all industries – BUSINESS absolutely BOOMING!!



In this role -- you'll manage/maintain all Unix / Linux systems to insure maximum uptime. You will design, drive and develop our new software features and more.



!!CRITICALTO BE CLEAR – we're looking for someone who is very skilled and has leadership in:

• Minimum of 5 years Linux Systems Administration

• Proficient in bash and perl or python

• Bare metal/vm full-stack app-server automation

• Expert level linux server networking, VLANs, bridges, and static routes

• Linux application debugging (identify blocking and inefficient system and application (C, perl, python, and java) processes

• Ability to tune linux kernel to optimize performance

• Rpm packaging (build, test, and deploy)

• Experience building and iterating through cobbler/kickstart, puppet modules, chef recipes, and shell/perl/python scripts

• Experience setting up database systems (e.g. replication, active/active)

• Knowledge of SQL, ability to query and join tables

• Experience with apache and nginx (proxy and reverse proxy configuration, server load balancing, and site configs)

• Experience provisioning public and private clouds (AWS, Rackspace, CloudStack, OpenStack, VMWare, KVM, and Xen)



++ GREAT PLUSES ++

• Support a Java-based application running on Tomcat

• Managing Cisco, Juniper or HP network equipment with a strong understanding of Layer 2 and Layer 3 networking technologies experience

• Optimizing BGP routing

• 2+ years with Amazon AWS (EC2, ELB, S3, Glacier)

• Digital advertising technologies experience

• Experience with 
Hadoop and related project

• Atlassian Jira and Confluence experience



**C2C/W2 contract workers are not preferred

** Must be on-site in our San Francisco City, CA offices



If this is sounding like you so far simply click and briefly apply! Be sure to INCLUDE your current RESUME if you email me directly!!

Top Reasons to Work with Us

- Up to 100% covered health care coverage and other great benefits

- GREAT COMPANY CULTURE - working collaboratively

- 401k matching plan, flex hours and up to 3 or 4 weeks of PTO!

What's In It for You

We're a great company that heavily values each new hire. If hired, you'll receive:

- Competitive Base Salary (target range $110k - $155k depending on your core skills related to this job)

- Up to 100% covered health care coverage and other great benefits

- GREAT COMPANY CULTURE - working collaboratively

- 401k matching plan, flex hours and up to 3 or 4 weeks of PTO!So, if you are a Linux / Site Reliability Engineer with experience, please apply today!

Applicants must be authorized to work in the U.S.

Please apply directly to by clicking 'Click Here to Apply' with your Word resume!

Looking forward to receiving your resume and going over the position in more detail with you.

- Not a fit for this position? Click the link at the bottom of this email to search all of our open positions.

Looking forward to receiving your resume!

CyberCoders

CyberCoders, Inc is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work - In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

Copyright &copy; 1999 - 2014. CyberCoders, Inc. All rights reserved.


Senior Hadoop Developer, Content Pipeline - Mountain View, CA - QuixeyQuixey is looking for a talented hands-on Senior Hadoop Big Data Developer with a strong academic track record and experience working on similar challenges. As a Senior Hadoop Big Data Developer you will work as part of our content pipeline team on building highly scalable content extraction pipelines. Quixey is tackling some very big and challenging problems in the domain of search and if you feel you are up to the challenge, we’d love to hear from you.As Hadoop Developer you will be:

 Developing Pig scripts and Java UDFs for extraction and normalization of content.
 Scaling our content pipeline to process larger data sets faster.
 Creating generic solutions for each vertical of data we are processing.
 Working closely with the content crawling team in identifying crawling targets.

As Hadoop Developer you should have:

 
  MS / BS Degree in Computer Science or equivalent industry experience.
  10+ years experience in Java development.
  5+Experience with Hadoop Big Data.
  Python experience a plus.
  Experience in Agile methodologies and processes a plus.
  Experience with AWS a plus.
 



Hadoop Developer. Please send resume to ravi@tekforcecorp.com with rate and location.
Requirements:

 Experience developing Develop, implementing, and refining data engineering solutions for huge volumes of data.
 Professional experience with Hadoop, Map/Reduce, Pig/Hive/Impala, Sqoop etc.
 Data Warehouse experience with Teradata, Oracle etc.
 Experience designing and implementing dimensional data models that scale across an enterprise business.
 Experience building scalable ELT/ETL workflows to transform and integrate data in to structures conducive for reporting and analytics in Hadoop.
 Familiarity and experience in different phases of software development life cycle.
 Good understanding of algorithms, data structures, performance optimization techniques, and object-oriented programming



Sricom, Inc is seeking Senior Hadoop resource for 6+ months in San Jose, CA
Qualification

 8 years of software development experience on large scale distributed systems
 Hands on experience with Kafka required
 Expertise with SQL scripting, data warehousing, business intelligence, and ETL development
 Familiarity with Hadoop open source stack including YARN, Kafka, Hive, Pig, Sqoop and RDBMS such as MySQL
 Hands on experience with AWS EMR, Dynamo, RDS, Redshift, cloud storage such as S3, Glacier, EBS, etc.
 Proficiency in programming languages like Python, Perl, Ruby, Scala, etc.
 Experience building large-scale distributed applications and services
 Experience with other cloud storage provider platform such as Google Cloud Engine, Azure bonus points.
 Excellent written/oral communication skills
 Proficiency in statistical analysis and data mining techniques a plus
 BS/MS - Computer or relevant engineering discipline strongly desired



Great oppourtunity,&nbsp;
&nbsp;Appreciate if you can reply with your resume in word along with contact details.
&nbsp;
Sr.Java Engineer/ Lead software Engineer
San Francisco
Contract: Long term
&nbsp;
As an engineer in our development team you will be responsible for the development and delivery of the web services supporting Company's user experiences on various channels (web, mobile, etc.), as well as integrations with partners. You will work closely with product and experience to understand the requirements and deliver the API that meets these requirements.
Essential Job Expectations

 Strong Java back End developer
 Experience in JMS
 Expert in REST.
 Provide technical leadership and mentoring to engineers within the team
 Familiar and comfortable with the technologies, the tradeoffs, and the design patterns emerging in web services
 Thrive in a fast-paced, dynamic environment. You have a bias towards action and results

Qualification:&nbsp;

 BS in Computer Science or related degree required. MS preferred
 Experience in developing high performance, scalable and available web services using J2EE and frameworks such as Spring, Hibernate.
 Knowledge of object-oriented design and design patterns, and programming skills
 Experience in designing, developing and delivering software and/or platforms used by other developers,
 Good understanding of web services and SOA related standards like REST/OAuth/JSON and SOAP/WSDL.
 Excellent knowledge of programming languages, especially Java.
 Excellent communication and writing skills

Warm Regards,
Mukta Goel
Tel| 408-845-9400 ex 105
Email: mukta@flextoninc.com


Please email resume to shri@intelliswift.com
Full time and long term contract positions - Senior Java Bigdata engineer
&nbsp;
1&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Principal – Big Data Hadoop Engineer (Rich experience on Hadoop Ecosystems and must have used Sqoop/Spark etc and must have rich Java J2EE background)
2&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sr. Software Engineer (Hadoop &amp; Map Reduce)


Mindsource specializes in placing consultants in Silicon Valley, CA. We are looking for enthusiastic Big Data Haddop Engineer&nbsp;to be a part of our client’s team. If you are match and interested in this position, please respond immediately to sowmya@mindsource.com&nbsp;( sowmya at mindsource dot com ) along with your&nbsp;updated resumeand give me a call at 650-314-6408.
Job Title : Big Data Engineer
Location: San Bruno,CA
Duration: 6 - 12+ months&nbsp;
Minimum Skills Required (at least 3-5 &quot;must haves&quot;)
- Strong development skills around Hadoop, Hive, Pig, HBase, Cassandra, Map Reduce
- Strong skills in Object oriented design, core Java, shell scripting/Python
- Solid foundation in data structures, and algorithms
- Proven background in Distributed Computing, Data Warehousing, ETL development, and large scale data processing
- Proven experience to lead a project team to complete complex projects from start to end
- BS + 8 years, MS + 6 years, PhD + 3 years of work experience in a similar role
- Excellent problem solving and analytical skills
- Ability to navigate and work through ambiguity
&nbsp;
Preferred Qualifications (3-5 &quot;nice to haves&quot;)
- Strong development skills around building scalable low latency web-services
- Strong development skills in Python and SQL
- Knowledge and proven experience on SOA,&nbsp; performance/scalability tuning, real time analytics
- Strong customer service skills and excellent verbal and written communication skills
- Experience as a scrum master and on agile methodology
- Good testing practices (unit, integration, system) with automation
&nbsp;
Thanks &amp; Best Regards,
Sowmya&nbsp;
Resource Manager | MindSource Inc
Work: 650-314-6408&nbsp;
email: sowmya@mindsource.com &nbsp;
&nbsp;


Hive Develoepr with&nbsp;Building data lake experience
San jose,CA
6Months+
Looking for Hive developer with Building data lake experience
&nbsp;
Send Profiles to sindhu@srsconsultinginc.com
&nbsp;


Role: Hadoop Developer/Architect
Location: San Francisco, CA, 94108
Duration: 6 Months +
&nbsp;
 Roles &amp; Responsibilities: 
1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8-10 years’ experience in developing software applications including: analysis, design, coding, testing, deploying and supporting of applications.
2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Proficient in application/software architecture (Definition, Business Process Modeling, etc.).
3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Project Management experience.
4)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Experience building Big Data solutions using Hadoop technology.
5)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Extensive experience with software development and the complete software lifecycle (analysis, design, implementation, testing, quality assurance).
6)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ability to work with non-technical resources on the team to translate data needs into Big Data solutions using the appropriate tools.
7)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Extensive experience developing complex MapReduce programs against structured and unstructured data.
8)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Experience with loading data to Hive and writing software accessing Hive data.
&nbsp;
Qualification Rating:

 Developer Build programs that leverage the parallel capabilities of Hadoop and MPP platforms * 3+ Yrs.
 Collaboration * 4 (Very Strong)
 Columnar DB solutions Vertica - Cassandra - Greenplum for Data Management * 3+ Yrs
 Hortonworks Hadoop distribution components and custom packages * 3+ Yrs.
 Loading external data to Hadoop environments using Flume * 3+ Yrs.
 Loading external data to Hadoop environments using MapReduce * 3+ Yrs.
 Loading external data to Hadoop environments using Sqoop * 3+ Yrs.
 PIG scripting to manage data * 3+ Yrs.

&nbsp;
Thanks and Regards,
Kiran
Infobahn Softworld Inc.
2010 N. 1st Street, Ste 470, San Jose, CA 95131
Phone # 408 413 1222 - EXT 208
 kiran@infobahnsw.com


Please send your resume to bdutta @ buxtonconsulting . com
Someone with the background to build solutions around real-time platforms that serve over billion of events a day. Someone excited about building solutions primarily based on Open Source projects.You will be responsible for:

 Build high-performance distributed systems for large-scale indexing, data quality, data collection and aggregation.
 Own the design, development, deployment, and ongoing support of major components
 Collaborate with Product Manager, engineers, QA, and operations teams to develop world class software using agile methodologies
 Good verbal and written skills. Enthusiasm and a willingness to dig in and learn on the job is a must
 Working with operations teams to ensure your applications and services are highly available and reliable

&nbsp;
Job Requirements

 BS/BA in Computer Science or related field 7+ years work experience or MS
 Very strong in system programming skills in Java
 Experience with Spring and Restful API, JSON
 Experience with Hadoop platform and core concepts like HDFS, MR, Hbase, Interactive querying is a must.
 Solid CS fundamentals, particularly in highly concurrent algorithms and data structures
 Ability to take a project from scoping requirements through actual launch of the project
 Ability to be flexible to handle multiple priorities and to work limited overtime as necessary
 Excellent analysis, design, and problem-solving skills

&nbsp;


Role: Bigdata Hadoop Admin
Description:
Hadoop administrator with&nbsp;Cloudera, Hive experience
Good communication skills
&nbsp;
Please send me your resume at srinivas@iitsolutionsinc.com
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
Srinivas
T:216-744-7169


Minimum Required Skills:
Java, Big Data, 
Hadoop, Cassandra, HBase, Solr, Lucene, NoSQL, Cloud, Analytics

We are a well established South Bay Area software product company rolling out state of the art web and Cloud based products used by enterprise business customers. 

We are looking for a Senior Java Developer with 
Hadoop/Big Data experience to join our team.

What You Will Be Doing

•Design and implement map reduce jobs to support distributed processing using java, hive and hbase
 •Build libraries, user defined functions, and frameworks around 
Hadoop
 •Research, evaluate and utilize new technologies/tools/frameworks around 
hadoop eco system such as MapReduce, HDFS, Hive, HBase, etc.
 •Develop user defined functions to provide custom hbase/hive capabilities
 •Work with Site-Operations team on upgrades of the cluster
 •Participating in the installation, configuration and administration of a single-node and multi-node 
Hadoop HDFS cluster

What You Need for this Position

At Least 3 Years of experience and knowledge of:
 - Java
 - Big Data
 - 
Hadoop
 - Cassandra
 - HBase
 - Solr
 - Lucene
 - NoSQL
 - Cloud
 - Analytics

What's In It for You

- Vacation/PTO
- Medical
- Dental
- Vision
- Relocation Assistance
- Bonus
- 401(kIf this sounds like a job you can do please apply today!

Applicants must be authorized to work in the U.S.

Please apply directly to by clicking 'Click Here to Apply' with your Word resume!

Looking forward to receiving your resume and going over the position in more detail with you.

- Not a fit for this position? Click the link at the bottom of this email to search all of our open positions.

Looking forward to receiving your resume!

CyberCoders

CyberCoders, Inc is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work - In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

Copyright &copy; 1999 - 2014. CyberCoders, Inc. All rights reserved.


FusionForte, a California based Information Technology services company is backed by industry professionals with a combined experience of over 100 years.FusionForte combines deep industry experience with technology expertise &amp; product knowledge to deliver consulting services in Business Intelligence, SOA, Application Engineering, Analytics,*BPM, ERP, CRM, HCM, SCM*and other business solutions.Hadoop Developer - San Francisco &amp; Dallas, TX - 6 Months
Summary Description:&nbsp;
The Hadoop Software Engineer will be responsible for building the Hadoop environment within the Integration Lab. Successful candidate will collaborate with architects and software engineers to install, test, and certify the interoperability of multiple Client’s products and partner products, and engage with professional services teams on customer-specific ecosystem integration efforts, both in the engineering lab and customer environment.
Key Areas of Responsibility (specialization in Hadoop product and technologies):

 Install and configure Hadoop products in the Ecosystem
 Contribute to detailed project plans and lead technical project scoping and planning;
 Design, and develop automated test cases that verify solution feasibility and interoperability, including performance assessments. Manage and execute complex,multi-platform test plans and procedures
 Document test case results, including product configuration information, and solution components/requirements to accompany a certified solution configuration
 Collaborate with other engineers to create an integration and regression testing framework for testing ecosystem environments.
 &nbsp;Mentor and train professional services staff in the construction of Big Data ecosystem solutions in a hands-on lab environment
 Using expertise developed in the lab environment, support customer installation and field deployment initiatives as necessary
 Providing recommendations/suggestions for improving testing efficiency
 Provide product usability and enhancement feedback to both product and partner development teams.
 Apply new technologies to problems; assess impact of technology changes

Work Environment:
This position is located in San Francisco.&nbsp;
Skills &amp; Attributes:
The ideal candidate will have great written and oral communication skills, cross-group collaboration skills, excellent analytical skills, and will have the ability to:

 &nbsp;grasp new technologies quickly
 &nbsp;solve complex technical problems
 &nbsp;multi-task and plan own work &amp; track to plan
 &nbsp;work in a fast pace environment with a strong sense of urgency

Qualifications
Basic Qualifications

 &nbsp;Bachelor’s degree in Computer Science or relevant, related experience
 &nbsp;3+ years of software development experience utilizing the following:
 Java, C, C++, C#, Perl, UNIX shell, SQL, Nosql, HTML, etc.
 &nbsp;Experience in building 'big data' processing framework
 &nbsp;Experience of working with Hadoop, Hive, Pig, Hbase &amp; Storm
 &nbsp;Experience in developing software on Linux

Preferences

 &nbsp;Advanced degree in Computer Science or relevant, related experience
 &nbsp;Deep understanding and hands-on experience with the overall Hadoop ecosystem
 &nbsp;SQL-HTM, Aster-Hadoop Adaptor
 &nbsp;Demonstrated analytical and problem solving skills; particularly those that apply to a Big Data environment.&nbsp;


 &nbsp;


 Please submit your resume via “Apply “ below. For more information email me at anu.chinnappa@fusionforte.com&nbsp;
 
 
 www.fusionforte.com&nbsp;

&nbsp;


Need ETL Informatica Developer with hadoop&nbsp;
San Jose &nbsp;CA
6 Months+
Skills:
&nbsp;
Need a &nbsp;ETL Informatica Data warehosuing consultant with experince on Big Data hadoop Technologies
&nbsp;
Regards,
Durga Prasad
SRS Consulting Inc.
Email:durgaprasad@srsconsultinginc.com


&nbsp;
&nbsp; RichRelevance powers personalized shopping experiences for the world’s largest and most innovative retail brands, including Williams Sonoma, Sears, Target and others. Founded and led by the e-commerce expert who helped pioneer personalization atAmazon.com, RichRelevance helps retailers increase sales and customer engagement by recommending the most relevant products to consumers regardless of the channel they are shopping. RichRelevance has delivered more than $5.5 billion in attributable sales for its retail clients to date, and is accelerating these results with the introduction of a new form of digital advertising called Shopping Media which allows manufacturers to engage shoppers where it matters most -- in the digital aisles on the largest retail sites in world. RichRelevance’s Shopping Media platform is a unique retail advertising platform that supports brand- and performance-based campaigns to provide advertisers the opportunity to target consumers as they shop large multichannel retailers such as Target, Sears and Home Depot.&nbsp; RichRelevance is headquartered in San Francisco, with offices in New York, Seattle, Boston, and London, and has been twice recognized as one of the “Best Places to Work” in the Bay Area. For more information, please visit&nbsp;www.richrelevance.com. &nbsp; Primary Responsibilities:

 Principle: 100% customer satisfaction!
 Development &amp; support of Hadoop and other infrastructure (Architecture experience a plus)
 Technical management, documentation and internal education of projects
 Software release/deployment (RPM, Puppet, etc).
 Troubleshooting/support of a large, worldwide web presence.
 Java application support.
 R&amp;D; test new technology and recommend architecture and software solutions.
 Working with Fortune 500/1000 companies.
 Participate in 24/7 on-call rotation.

&nbsp; This position will specifically be responsible for:&nbsp;

 Increasing efficiency, speed of delivery, and reliability at every level of RichRelevance service.
 Facilitate access to large data sets utilizing Hive/Hbase and custom solutions on Hadoop CDH4 and above.
 Identifying and vetting next-generation technologies to incorporate into our products.
 Scaling RichRelevance infrastructure to support and utilize petabytes of data.
 Assisting in support of internal and external customers on a variety of issues and projects.

&nbsp; Our ideal candidate will have the following skills and capabilities:&nbsp;

 BS/MS in CS or Engineering discipline or equivalent experience.
 Linux administration (Centos 5+, RHEL).
 Hadoop and related technologies (MapReduce, HBase, Hive, Avro, Presto)
 Cloudera Distribution and Administration experience a plus
 Great communicator—can speak at both a technical and functional level to internal and external customers.
 3-5 years production internet SaaS systems administration.
 Experience implementing opensource packages. Active contribution a plus.
 Working knowledge of SQL, familiarity with PostgreSQL administration.
 Scripting utilizing BASH, Perl, or Ruby.
 Experience dealing with security or privacy concerns is a plus.
 Team player—it’s a small team, and when needed at 1:00 AM, we work it together.

&nbsp; RichRelevance is an Equal Opportunity Employer and does not discriminate against any applicant on the basis of race, color, religion, national origin, gender, marital status, age, disability, sexual orientation, military/veteran status, or any other status protected by Federal or State law or local ordinance. &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;


We have urgent requirement for&nbsp;Java Engineer:Multiple&nbsp;positions:&nbsp; 
San Francisco, CASan Jose,CA&nbsp; 
We are looking for exceptionally strong Core Java skills
Must have skills:
* Strong Java/J2EE, concepts of OOA/OOD (UML, Design Patterns)* Core Java/J2EE, Servlets, EJB, RMI, JDBC, JSP, Struts, Spring, Hibernate, JMS, XML/XSL/XSLT, XPath, SAX and DOM* Multi-Threading and synchronization* Caching and memory management * Extensive experience with Service Oriented Architecture and Web Services, WSDL, SOAP, UDDI, JAX-RPC, JAX-WS, JAXB, JAXR, JAXP, SAAJ.* Experience working with HADOOP and Databases like HBase and Cassandra* Good knowledge of Design patterns such as MVC. * Experience on any App server preferably Weblogic/Websphere/JBoss/Apache Tomcat* Experience on very high volume transactional eCommerce applications. * 3 to 5 years of experience on design and development over Java/J2EE Platform
&nbsp;*Experience working on Automation Test scripts and tools is a plus ( ex JMeter, JBehave)

 Desirable:&nbsp; * Knowledge of Log4J,Junit* Systems analysis and design experience. * E-commerce domain knowledge.
 Bacholers or Masters in Computer Sc and Engg is plus



&nbsp;Hadoop Solution Architect - DSSD
DSSD is a Menlo Park, California-based developer of an innovative new rack-scale flash storage tier that is now part of EMC’s Emerging Technology Products Division. DSSD’s architecture complements EMC’s industry-leading flash storage portfolio and is designed to deliver game-changing performance for I/O-intensive in-memory database and Big Data applications such as SAP HANA and Hadoop. &nbsp;DSSD will operate as a standalone unit in EMC’s Emerging Technology Products Division.&nbsp;
Job description
The Hadoop Solution Architect is responsible for conceptualizing, planning, designing and implementing complete and integrated Solution Architectures that support DSSD products alongside Hadoop and other Big Data applications and DB’s. The solution architect will explore and build early-stage, differentiated solutions that leverage DSSD’s truly unique capabilities. The solution architect will work closely with R&amp;D, Global Services, Sales Engineering, Product Management and Product Marketing and be the subject matter expert and ultimate go-to person in a number of important use cases. As a solution architect, a deep understanding of technology infrastructure and how it can be used to apply to an IT organization and business is required. This individual can clearly articulate the benefits and relevance of the concepts, products and technologies related to their architecture domain to a diverse audience. This role includes extensive interaction with customers, partners, and industry sources.
The candidate will work with technology partners to find technical integration points to drive a more compelling solution, articulating to potential partners the value of investing technical resources for testing and documentation, jointly working with partner on a test plan with clear milestones and delivery dates, and hands on in building out test environments and the documentation of technical evidence in the form of reference architectures, best practices guides, white papers, solution briefs, etc.
Desired skills

 Hands-on, “can do” attitude.
 Customer focused
 Flexible
 Ability to multi task
 Expert in RedHat Linux system administration
 Experience with distributed computing systems
 Experience with administration of large scale RDBMS or data warehouse systems
 ETL, ELT and/or reporting experience
 Demonstrated ability to apply problem analysis and resolution techniques to complex system problems
 Strong network background with a good understanding of TCP/IP, firewalls and DNS
 Strongly Desired: Hadoop installation and system administration
 Desired: Ability to read, understand, and build Java code
 Desired: Scripting expertise including BASH, PHP, PERL, Java script and UNIX Shell
 Strong executive presence with the ability to evangelize new products, technologies, and value propositions.
 Must be articulate, a strong writer and presenter and have the ability to interact with the market through extensive meetings with customers, partners, press/media and industry sources to evangelize DSSD’s solutions and collect feedback.
 Experience creating reference architectures, whitepapers, performance benchmarks and other technical content.
 Forge strong relationships with the sales force and become their trusted partner and subject matter expert.

Education/experience

 Minimum 10 years of solution architecture or related experience in storage or closely related markets.
 CS or Electrical Engineering (or equivalent) degree - required
 Experience in one or many of the leading Hadoop distributions and building large-scale data mining or analytics solutions on these technologies

When you choose DSSD, you join a diverse world of innovative thought leaders. At our core is a commitment to workplace diversity, the sustainability of our planet, and the community corporate involvement. We offer highly competitive salaries, bonus programs, world-class benefits, and un-paralleled growth and development opportunities – all to create a compelling and rewarding work environment.
We are an Equal Employment Opportunity employer that values the strength diversity brings to the workplace. All qualified applicants regardless of race, color, religion, gender, sexual orientation, marital status, gender identity or expression of national origin, genetics, age, disability status, protected veteran status, or any other characteristic protected by applicable law are strongly encouraged to apply.
DSSD does not accept unsolicited Agency Resumes. DSSD will not pay fees to any third part agency or firm that does not have a signed “EMC Agency Fee Agreement”


Job Description:
We are seeking a candidate with experience in developing, implementing, and refining data engineering solutions for high volumes of data. The candidate should be familiar and experienced with the different phases of the life cycle of software development and have a good understanding of algorithms, data structures, performance optimization techniques, and object-oriented programming.
Required:

 Experienced in Data Warehouse with Teradata, Oracle, etc.
 Experience with designing and implementing dimensional data models that scale across an enterprise business
 Experienced with Hadoop, Map/Reduce, Pig/Hive/Impala, Sqoop, etc.
 Experience with building scalable ELT/ETL workflows to transform and integrate data into structures conducive for analytics and reporting in Hadoop

For immediate consideration, please send your resume to tyler@erginc.com.




Responsibilities 

Symantec's Cloud Platform Engineering group is responsible for the strategic architecture and deployment of our vast applications and infrastructure ecosystems that are at the core Symantec's Cloud offering. A Platform Engineering role in Symantec's Big Data team is an expert in the Big Data space but also has an equal passion and understanding for DevOps. As a key member of the Big Data team you will be responsible for architecting, designing and developing major components of next generation data platform in Symantec. You will be involved in the software development lifecycle from strategy, architecture, design, implementation, testing and release with a focus on delivering solutions against business goals, technical requirements and Symantec engineering standards. 

Successful candidates for these openings have extensive experience engineering and maintaining applications and infrastructure in large-scale, highly-available cloud environments. They will: 

 * Participate/drive the deployment of new security applications the run in Symantec's cloud infrastructure 

 * Develop/deploy infrastructure components the support those applications 

 * Responsible for the reliability and scalability of the applications and cloud environment 

 * Consistently engineer their services to make them run better in the production environment 


Qualifications 

* BS degree in Computer Science or related field * 3-5 years of engineering experience in similarly scoped and complex environments. 

 * You love to code in Java, Python or C++. 

 * Test driven development experience. 


Ideal competencies: 

 * Big Data/
Hadoop 

 Openstack 

 * Apache Cassandra 

 * NoSQL 

 * Experience in continuous integration environments. 

 DICE-CPE 

 *LI-CPE 

 IND1


Conducts analysis, designs products, and programs computer software that requires extensive research. Typical background for this position is advance study/knowledge in the field of computer science or software engineering along with advanced knowledge of software development and methodologies. |

 Write high-quality, high-performance, maintainable code
 Write high-quality, well-performed server based code and APIS using JAVA
 Participate in design and code reviews; identify best practices

Required Skills: 

 Experience writing multi-threaded, asynchronous code
 Good understanding of web technologies, network protocols and Linux
 Experience with Spring/Hibernate/ActiveMQ/JBoss/Oracle
 Experience with building REST APIs and service-based architectures.
 Disciplined and passionate about Software Engineering
 Able to drive architecture and design
 Able to influence and direct other groups as well as team
 Strong communication skills; attention to detail and discipline

Desired Skills: 

 Experience with OAuth
 Experience with Hadoop and HBase

Additional Comments: 

 Desired Industry background: Technology.
 Not looking for Team lead, or Project Managers or program managers.
 Looking for Strong “Java engineers” that can lead projects from start to end.

Interview Process: Interview process includes hands on writing of Java code
Thanks
Karthik SVVS • Senior Executive – Recruitment
Ness Software Engineering Services
1000 Town Center Way, Suite 210, Canonsburg, PA 15317
Office: (724) 514-3406 | Fax: (724) 918-9230
shanmugakarthik.saraswathi@ness.com | www.ness.com | &nbsp;|&nbsp;http://blog.ness.com
twitter.com| linkedin.com | facebook.com


Position: 
Hadoop developer
 Location: Pleasanton,CA


Job description:
 * Experience in 
Hadoop Technologies - HDFS, Map Reduce, Hive, Zookeeper, H-base, Flume, Cloudera manager .
 * Good working knowledge of Pig Scripting, Oozie workflow and HBASE.
 * Define and develop client specific best practices around data management within a 
Hadoop environment.
 * Hands on experience on Core Java in Linux environment.
 * Work with functional team/ Frameworks team to understand requirements and translate them to technical specifications.
 * Cloudera certified is preferred.
 Organization Marketing Statement
 Cognizant (NASDAQ: CTSH) is a leading provider of information technology, consulting, and business process outsourcing services, dedicated to helping the world's leading companies build stronger businesses. Headquartered in Teaneck, New Jersey (U.S.), Cognizant combines a passion for client satisfaction, technology innovation, deep industry and business process expertise, and a global, collaborative workforce that embodies the future of work. With over 50 delivery centers worldwide and approximately 171,400 employees as of December 31, 2013, Cognizant is a member of the NASDAQ-100, the S&amp;P 500, the Forbes Global 2000, and the Fortune 500 and is ranked among the top performing and fastest growing companies in the world. Visit us online at www.cognizant.com or follow us on Twitter: Cognizant.


Qualifications:
 Bachelors degree in Engineering or Science graduates with 4-7 years of experience.
&nbsp; 


EmployeeStatus:  Permanent 

JobType:  Standard 

JobShift:  Day Job 

PostingDate:  Sep 5, 2014, 3:57:15 PM 

WillTravel:  No 

Department:  Application Development / Application Maintenance


Varite Inc is looking for Hadoop Architect for one of its client in Sunnyvale, CA
Here are the Job Details:
Title: Architect 
Duration: 7 Months Contract 
Location: Sunnyvale, CA
Job Requirements Define procedures, process, and tools for system builds, deployments, code migration, and integration across the ASUP Backend landscape. • Application installation, configuration, standardization, load-balancing, high-availability configuration, deployments, migrations, capacity planning and refreshes • Support a scalable, extensible, reliable, maintainable infrastructure, optimizing hardware and software configurations, and enabling rapid deployment of new instances. • Install new software releases, system upgrades, evaluate and install patches and resolve software related issues. • Work with Architecture and Development teams to test and benchmark new versions, patches and software components for functionality and reliability. • Work with the project team to automate management tasks, streamline processes and perform standard administration functions as needed. • Ongoing corrective and preventative maintenance to improve reliability, availability and performance • Preparation of operational documentation and provide TOI to operations as part of Go-live. • Provide L3 Technical Support and participate in P1 incident triage/resolution as required. • Design technical solutions and own key decisions to maximize the performance, uptime, and supportability of applications and technology platforms. • Provide expert-level system analysis and problem resolution. • Manage technical requirements, documentation, and deployment of custom development efforts. • Maintain comprehensive project, technical, and architectural documentation for enterprise systems. • Responsible for updated disaster recovery plans and the execution of business continuity recovery tests. • Contribute to the selection, prototyping, implementation and support of tools for system monitoring for large scale data environments.
Education and Experience: 6-8 years • Master level experience with large scale Hadoop environments build and support including thought leadership, design, configuration, installation, capacity planning, capacity management, and performance tuning and monitoring. • Expert level knowledge of Hadoop eco-system, Cloudera EDH experience preferred, (HDFS, MapReduce, HBase, ZooKeeper, Flume etc) • Hands-on practitioner in Architecting, Implementing and Operating Applications built on NOSQL (Cassandra / MongoDB)and Lucene Search (Datastax Enterprise experience strongly preferred). • Strong scripting skills (Bash, Perl, Python, etc). • Knowledge and experience with parallel ETL (Pentaho Suite for ETL and Data Integration), • Experience with infrastructure automation/config management tools (familiarity with chef/puppet a plus) • Solid experience with monitoring frameworks (Cloudera Manager, Ganglia), and analyzing production platforms, UNIX servers and mission critical systems with alerting and self-healing systems • • Must be familiar with the basics of MySQL on Linux (basic config, necessary services, basic SQL knowledge). • &nbsp;Exceptional written and interpersonal communications skills with the ability to clearly articulate technical problems and solutions in business context. • Strong organization, facilitation, and leadership skills. • Capacity to work under pressure and manage multiple tasks simultaneously. Hadoop, Datastax, Cassandra, Hbase, Cloudera, Pentaho
Nice to have's:
5+ years experience with Java and related technologies.
5+ years of Unix/Linux expertise ( architecture, systems administration and UNIX tools)
Keywords:architect, Hadoop, Datastax, Cassandra, Hbase, Cloudera, Pentaho, java, unix, linux,&nbsp;architect, Hadoop, Datastax, Cassandra, Hbase, Cloudera, Pentaho, java, unix, linux,&nbsp;architect, Hadoop, Datastax, Cassandra, Hbase, Cloudera, Pentaho, java, unix, linux
Search &amp; apply for all the open jobs with VARITE: http://www.varite.com/career.htm
&nbsp;


Hello
Our client is looking to fill multiple positions for&nbsp;Java Developer with Hadoop/BigData/Mapreduce.
Role Java Developer with strong in Bigdata/Hadoop/Mapreduce Programming.
Total Experience Required 6+ Years
Work Location Multiple positions in USA (CA/NJ/TX/AR/IL etc)
Must have skills:&nbsp;Core java/J2EE,mapreduce,bigdata,hadoop,hive,nosql,hbase,mongodb,R,python,analytics,Core java/J2EE,mapreduce,bigdata,hadoop,hive,nosql,hbase,mongodb,R,python,analytics.
Work on Big data platform based on Hadoop ecosystem including Core java/J2EE,PIG, Oozie Workflow, SQOOP, HIVE &amp; Map Reduce tools,R &amp; Python.
Send in your queries to&nbsp;rajesh.k@candi-dus.com
Thanks


Sr. 
Hadoop Infrastructure Engineer 
Redwood City/ CA
(Full time employee position directly with a client of Tiva Systems)


Responsibilities:

* 
Hadoop ecosystems deployment, management and monitoring,
* 
Hadoop ecosystems performance tuning and capacity planning
* 
Hadoop ecosystems Security
* 
Hadoop Infrastructure Engineer
* Big Data operations 

Qualifications:

* Expert knowledge of 
Hadoop administration, HDFS
* Expert knowledge of Linux administration CentOS
* Expert knowledge of scripting in any of Puppet, Bash, Perl, Python, Ruby
* Experience with Flume, Storm, Kafka and other data ingestion technologies
* Experience working in Cloud based systems. 
* Experience in using Git, Maven, Jenkins &amp; and other CI tools
* Working knowledge with Hbase, Cassandra, Hives, Cloudera or similar products
* Analytics, Big Data, Cloud Computing and High Performance Computing.
* Architecture, Deployment and operational leadership for multiple HPC and 
Hadoop environments greater then 4000 nodes.
* Architectural lead for defining Disaster Recovery and Business Continuity capabilities for 
Hadoop, Oracle and OpenStack solutions
* Cloudera Certified Administrator for Apache 
Hadoop - CCAH
* Mapreduce, Pig, Hive, Flume, ZooKeeper, HDFS

Keywords: - Graphite, OpenTSDB, Logstash, Elastic Search, Jira, Confluence, Stash, OpenGrok, Gerrit, Puppet/InfraDB, Sonar, Oozie, kafka, scribe, 
Hadoop, HBase

Please send your resume to hr@tivasys.com


Hello,
Greeting for the day,
Please find the below Big Data / Hadoop Job Details and let me know your thoughts/interest for any of the position which is best fit for you with a copy of your most updated resume,
&nbsp;
Job Title: Hadoop Architect / Hadoop Admin / Data Scientist /Hadoop Engineer 
Duration:&nbsp; Full Time/Permanent
Compensation:&nbsp; Open
Location : Bentonville, AR / Columbus, IN / Cupertino, CA/ Santa Clara, CA / Warren, NJ / Houston, TX
&nbsp;

 
  
   Job Title
   :
   Hadoop Engineer 
  
  
   Relevant Experience (Yrs)
   :
   8+ overall experience 2 years Hadoop experience &nbsp;
  
  
   Technical/Functional Skills
   :
   Hadoop Engineer &middot; &nbsp; &nbsp; &nbsp; &nbsp; 8+ years overall IT experience. &middot; &nbsp; &nbsp; &nbsp; &nbsp; 2 years of experience with Big Data solutions and analytical techniques. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Data Analysis/Scientist experience in the Big Data space. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Knowledge of the Hadoop ecosystem with experience in Hive, MapReduce, etc. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Experience with reporting and analytic tools such as Datameer, Platfora, R, Mahout, MicroStrategy, SAS, SPSS. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Experience creating machine learning, data mining, data visualization, or statistical models for Big Data problems. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Experience with terabyte or larger size poly-structured data sets. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Experience with statistical analysis software, such as Revolution R, Matlab, SAS, SPSS. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Any experience in Hadoop (Cloudera), ETL (Talend, AbInitio), MapReduce, Hive, Pig, HBase, &middot; &nbsp; &nbsp; &nbsp; &nbsp; Experience with full Hadoop SDLC deployments with associated administration and maintenance functions. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Experience developing Hadoop integrations for data ingestion, data mapping and data processing capabilities. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Good interpersonal with excellent communication skills - written and spoken English. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Able to interact with client projects in cross-functional teams. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Good team player interested in sharing knowledge and cross-training other team members and shows interest in learning new technologies and products. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Ability to create documents of high quality. Ability to work in a structured environment and follow procedures, processes and policies. &middot; &nbsp; &nbsp; &nbsp; &nbsp; Self-starter who works with minimal supervision. Ability to work in a team of diverse skill sets and geographies
  
  
   Roles &amp; Responsibilities
   :
   &nbsp; &nbsp;
    
     You will be a thought leader on a Software Engineering team and help the team design and architect a flexible and scalable data pipeline that can ingest data from thousands of companies.
     You will design and architect a HDFS storage pattern for all data coming into the company.
     You will work with other software engineering teams to help them solve their data ingestion and ETL problems.
     You will design and architect near real-time data analytic pipelines for creating analytic Dashboards.
     You will help lead the team around potential hurdles and potholes that come with processing data at terabyte scale.
     You will use all the tools in your tool belt where they make sense; &nbsp;Hive, Flume, Hbase, Pig, Avro etc.
     You will help our Analytics team figure out how to perform analytic computations on a distributed Hadoop cluster.
     You will design and architect a solution that allows us to keep data in a Hadoop cluster in sync with other databases.
     You will write scripts and programs to process and move large volumes of data. &middot; &nbsp; &nbsp; &nbsp; &nbsp; You will design and architect an enterprise meta-data management solution that allows our analysts and business operations teams to easily find and query data
    
  
  
   Generic Managerial Skills
   :
   Presentation skills, Issue management, Analytical skills
  
  
   Education
   :
   B Tech or any Equivalent
  
  
   Work Location &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
   :
   Warren, NJ 
  
 

&nbsp;
&nbsp;

 
  
   Role 
   Data Scientist
  
  
   Total Experience Required 
   5+Years
  
  
   Work Location 
   Bentonville, AR/ Columbus, IN / Houston, TX
  
  
   &nbsp;
   3
  
  
   Mandatory Technical Skills 
   5 Years expertise in R, Python and SAS. 5 Expertise in Time series, decision trees, random forests, linear regression, collaborative filtering, clustering and other modeling techniques 5 Expertise in market Basket analysis, Expertise in implementing Analytical models on Hana platform Expertise in streaming analytics, 3 Experience in Hadoop Ecosystem 5 years Experience in Agile methodology, 5 years experience in RDBMS, Experience in Eclipse Experience in git/SVN eclipse plugin
  
  
   Desirable Technical Skills 
   Hadoop
  
  
   &nbsp;
   &nbsp;
  
  
   &nbsp;
   &nbsp;
  
 

&nbsp;

 
  
   Role
   Hadoop Administrator
  
  
   Total Experience Required
   10 years
  
  
   Work Location
   Hoffman Estates, IL / Cupertino, CA
  
  
   &nbsp;
   &nbsp;
  
  
   &nbsp;
   &nbsp;
  
  
   &nbsp;
   &nbsp;
  
  
   Mandatory Technical Skills
   3+ years supporting open source Linux operating systems (RHEL, CENTOS, Fedora) and hardware in an enterprise environment 3+ years experience installing, configuring Linux based systems Strong scripting expertise including BASH, PHP, PERL, Java script and UNIX Shell Knowledge of java virtual machines (JVM) and multithreaded processing Expertise in typical system administration and programming skills such as storage capacity management, performance tuning, system dump analysis, server hardening (security) Strong network background with a good understanding of TCP/IP, firewalls and DNS
  
  
   Desirable Technical Skills
   Hands on experience with the Hadoop stack; MapReduce, Sqoop, Pig, Hive, Flume) Knowledge of NoSQL platforms Hands on experience with deploying and administering MySQL databases Ability to read, understand, and build java code Hands on experience with opens source monitoring tools including; Nagios and Ganglia
  
  
   Mandatory Functional Skills
   Demonstrated ability to apply problem analysis and resolution techniques to complex system problems
  
  
   Desirable Functional Skills
   To implement new business initiatives as they relate to Hadoop
  
 

&nbsp;
Role &nbsp;&nbsp; &nbsp;Big Data Solutions ArchitectTotal Experience Required &nbsp;&nbsp; &nbsp;10 yearsWork Location &nbsp;&nbsp; &nbsp;Hoffman Estates, ILMandatory Technical Skills &nbsp;&nbsp; &nbsp;&quot;Hadoop-related architecture experienceArchitecting Big Data hardware/software deploymentsDemonstrable experience of NoSQL: Hadoop/HBase/HDFS, Cassandra, MongoDB, and RiakExposure to developing solutions using continuous integration&quot;Desirable Technical Skills &nbsp;&nbsp; &nbsp;&quot;Good knowledge of Unix/Linux, Ruby, Python, Perl or other scripting languagesDevelopment knowledge of J2EE and design patterns in a distributed, multi-platform, heterogeneous computing environmentGood knowledge of SQL, RDBMS platforms and Teradata&quot;Mandatory Functional Skills &nbsp;&nbsp; &nbsp;&quot;Self-starter with a passion for new technologiesMust have Experience in a technical leadership role&quot;Desirable Functional Skills &nbsp;&nbsp; &nbsp;Active participation the open source communities
&nbsp;

 
  
   1
   Role
   Big Data Architect
  
  
   2
   Required Technical Skill Set
   
    
     Core Java, J2EE , Design Patterns ,UML , Databases
     Hadoop – HDFS, MapReduce, HBase, Hive
     NOSQL - Cassandra,&nbsp; MongoDB, Greenplum
     Distros – Cloudera, IBM BigInsights, Pivotal, HortonWorks, MapR etc
    
  
  
   3
   No of Requirements
   5
  
  
   4
   Desired Experience Range
   10 + years 
  
  
   5
   Location of Requirement
   Santa Clara, CA&nbsp; ( Candidate should be also open to travel as per request )
  
 

&nbsp;
&nbsp;

 
  
   Desired Competencies (Technical/Behavioral Competency)
  
  
   Must-Have
   
    
     10+ years of hands on experience in Java/J2EE development. 2+ years of expertise in Big data technology stack. 3-5 yrs of experience in an architect role of which 1 year in a big data environment
     Architected and delivered at least 2 end-to-end enterprise big data solutions on the Hadoop/JAVA/J2EE stack ranging from scripting, middleware and Integration and Enterprise components for an n-tier distributed solution.
     Knowledge of the next generation technologies – Databases, Hadoop, MapReduce, NoSql DBs , Social Media, Cloud applications , Advance Analytics and information management&nbsp; etc.
     Experience in SOA and SOA related technologies
     Ability to architect solutions by mapping customer business problems to end-to-end technology solutions. Seeks out accountability and responsibility. Will take the lead when required
     Exceptionally demonstrated consulting skill. Effectively communicates and engages with customers (internal and external)
     Balance strategic development requirements with tactical delivery pressures and willing travel frequently including overseas travel.
    
  
  
   Good-to-Have
   
    
     Mix of advanced technology and strategic business acumen-both broad and deep technology and industry knowledge
     Effective in fostering executive level relationships
     Resourceful, confident under pressure, and has demonstrated skill in both expectation and crisis management
     Open source contributions to Big data space
    
  
 

&nbsp;

 
  
   SN
   Responsibility of / Expectations from the Role 
  
  
   1
   
    
     Working with world’s largest pool of big data experts with expertise on industry leading distributions and toolsets.
     Work closely and contribute to TCS’s world class digital team consisting of senior architects, Data scientists and smart engineers spread across the globe.
     You will drive brilliant projects and you will be keen to expand your work experience, contribute to unique and mould breaking approaches in Big data
     Participate in the development and delivery of, world -class innovative solutions for defined and undefined opportunities.
     Drive the Requirement Analysis, architecture, design, development, integration, delivery and evolution for different projects.
     Work closely with business development, customers’ technical architects and engineering &amp; development teams.
     Participate and prepare most compelling content, data points, solution, model and asset for proposals and lead the activity in order to guide the team to a winning proposition.
     Project Scoping, Estimation, Resource Planning, Scheduling and Quality Planning.
    
  
  
   2
   &nbsp;
  
  
   3
   
    
     Follows industry trends, both commercial and technology related
     Technical R&amp;D and Consulting.
    
  
 

&nbsp;
&nbsp;

 
  
   Role 
   Hadoop on Cloud Architect@ Santa Clara, CA
  
  
   Total Experience Required 
   5 - 7 yrs 2 yrs in Hadoop
  
  
   Work Location 
   Santa Clara, CA
  
  
   Mandatory Technical Skills 
   
    
     Primary Skills: Hadoop, Architecture - Complete detailed design of the Hadoop solution, including data ingestion, data storage, data transformation and data extraction - Conduct reviews of the Hadoop design with technical team members to validate the design and gather feedback -&nbsp; Hive, Pig, HDFS,&nbsp; Cloudera CDH, Hadoop MapReduce, Impala, AWS - Well versed in NoSQL and SQL Database Design with Hive, HBase - Experience writing UDF, UDAF and UDTF’s for Hive and Pig. - Ability to design solutions independently based on high level architecture - Define and document relational, machine, semi-structured and unstructured data sets ingested. - Propose when various methods available should be used. Time based, event based, columnar files, etc. -Define and document the update process for each data set ingested -Define the default metadata with each ingestion -Define process, methodology and tools to validate updates in ingestion and any additional file loads (Parquet) for quantity, quality and performance. -Identify alternate analysis and/or analytics tools that may be required for advanced queries and/or performance -Define anonymization requirements for data set for broader use vs. restriction/security
    
  
  
   Desirable Technical Skills 
   &nbsp;
  
  
   Mandatory Functional Skills 
   
    
     Secondary Skills:&nbsp; • Excellent communication skills (both written and verbal) with strong presentation and facilitation skills • Demonstrated ability to influence and consult (providing options with pros, cons and risks) while providing thought leadership to sponsors/stakeholders in solving business process and/or technical problems
    
  
 

&nbsp;
&nbsp;
BigData Lead_Bentonville, AR
&nbsp;
Skills Required: 10 years of Java/JDBC/Junit, 3 Years of Map-Reduce experience, Hadoop Streaming, 3 years of sqoop, 2 Years of Flume,SQL, HAWQ, HQL &nbsp;expertise,Yarn expertise, 5 Years Unix Shell Scripting Experience and Experience in working with JSON, XML,4 years experience in Maven 15 Years in SDLC and 3 years in Agile development Experience in Cloudera/Pivotal HD,Experience in Eclipse and Experience in git/SVN eclipse plugin
&nbsp;
I would highly appreciate if you could reply me with a copy of your most updated resume, also please send me the required information too along with your resume,
&nbsp;
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Your Full Name:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Email:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Phone&nbsp; No. :
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Current Location:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are you Open to Relocate:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Position Applied For :
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Notice Period/ Availability after Offer:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Work Permit:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Current Compensation(Base+ Bonus):
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Expected Compensation:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are you open to travel (If Yes how much %):
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do you have any other Offer:
&nbsp;
&nbsp;
Looking forward to work with you..
&nbsp;
&nbsp;
Thanks and Regards
Maneesh Singh
Email:maneesh@idctechnologies.com
&nbsp;
&nbsp;
&nbsp;
&nbsp;


Minimum Required Skills:
Java, Big Data, 
Hadoop, Cassandra, HBase, Solr, Lucene, NoSQL, Cloud, Analytics

We are a well established South Bay Area software product company rolling out state of the art web and Cloud based products used by enterprise business customers. 

We are looking for a Senior Java Developer with 
Hadoop/Big Data experience to join our team.

What You Will Be Doing

•Design and implement map reduce jobs to support distributed processing using java, hive and hbase
 •Build libraries, user defined functions, and frameworks around 
Hadoop
 •Research, evaluate and utilize new technologies/tools/frameworks around 
hadoop eco system such as MapReduce, HDFS, Hive, HBase, etc.
 •Develop user defined functions to provide custom hbase/hive capabilities
 •Work with Site-Operations team on upgrades of the cluster
 •Participating in the installation, configuration and administration of a single-node and multi-node 
Hadoop HDFS cluster

What You Need for this Position

At Least 3 Years of experience and knowledge of:
 - Java
 - Big Data
 - 
Hadoop
 - Cassandra
 - HBase
 - Solr
 - Lucene
 - NoSQL
 - Cloud
 - Analytics

What's In It for You

- Vacation/PTO
- Medical
- Dental
- Vision
- Relocation Assistance
- Bonus
- 401(kIf this sounds like a job you can do please apply today!

Applicants must be authorized to work in the U.S.

Please apply directly to by clicking 'Click Here to Apply' with your Word resume!

Looking forward to receiving your resume and going over the position in more detail with you.

- Not a fit for this position? Click the link at the bottom of this email to search all of our open positions.

Looking forward to receiving your resume!

CyberCoders

CyberCoders, Inc is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work - In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

Copyright &copy; 1999 - 2014. CyberCoders, Inc. All rights reserved.


Pl submit word resume with daytime contact # to Lena@Kshunya.com Thank you!
PERM ROLE- CONSULTATIVE&nbsp;NOT&nbsp;CONTRACTING for a Leading Data Implementation and Analytic Practice
For West Coast-&nbsp;Travel to West Coast required Mon-thurs
Clients in SFO area
&nbsp;**Can be based of anywhere in fron Chicago- West Coast**
Base + Bonus+ travel expenses
Entrepreneurial Data Transformation Engineers (Python, SQL, ETL, Hadoop and Vertica (desired)
 with strong hands-on data integration implementation experience.&nbsp; Our clients have large amounts of data to process and we are looking for individuals that think outside of the box to come up with unique Data &amp; Analytic solutions.&nbsp;
Skills

 5+ years of Data Warehouse implementation experience.


 Strong SQL scripting experience to analyze, transform and integrate high&nbsp;volume, complex data sources


 Solid experience in CUSTOM ETL design and implementation in a Map Reduce or MPP system in a data warehousing space (prefer Hadoop/Hive and Vertica experience).


 Strong UNIX working experience


 Experience with object-oriented programming languages&nbsp; (Python preferred)


 Ability to analyze high volume data to identify deliverables, gaps and inconsistencies


 Experience creating UDFs for data transformation


 Passion to build business driven, data solutions regardless of technology

Pl submit word resume with daytime contact # to Lena@Kshunya.com Thank you!


Primary Job Requirements
Conducts analysis, designs products, and programs computer software that requires extensive research. Typical background for this position is advance study/knowledge in the field of computer science or software engineering along with advanced knowledge of software development and methodologies. |
&nbsp;
The following are the technical requirements:
- Java Server Development
- JavaScript Development desired
- Perl/Python or any other scripting development
- Relation Database Schema Design and Development (MySQL preferred)
- Knowledge/Experience with Big Data (Hadoop/HDFS/Storm/HBase) and Cloud system will be beneficial
&nbsp;
The following are also the optional experience that this position will be helpful but not a must.
- Machine learning with approaches like Decision Trees and Representation Learning, etc....
- Data Mining. (i.e. Classification, Decision Trees, Linear Regression with Multiple Variables)
&nbsp;
Thanks &amp; Regards,
Siva
intelliswiftSoftware Inc., 2201 Walnut Avenue, Fremont, CA – 94538
siva@intelliswift.com | Phone: (510) 516-7425 | http://www.intelliswift.com


Actian&nbsp;transforms Big Data into business value for&nbsp;any&nbsp;organization―not just those with considerable resources. Actian provides transformational business value by delivering actionable insights into new sources of revenue, business opportunities, and ways of mitigating risk with high-performance in-database analytics complemented with extensive connectivity and data preparation. Actian Analytics Platform delivers extreme performance with off-the-shelf hardware, overcoming key technical and economic barriers to broad adoption of Big Data. Actian also makes Hadoop enterprise-grade by providing high-performance ELT, visual design,&nbsp;and SQL analytics on Hadoop without the need for MapReduce skills. Among tens of thousands of organizations benefiting from Actian are innovators using analytics for competitive advantage in industries such as financial services, telecommunications, digital media, healthcare,&nbsp;and retail.&nbsp;The company is headquartered in Silicon Valley and has offices worldwide.&nbsp;Stay connected with Actian Corporation at&nbsp;www.actian.com&nbsp;or on&nbsp;Facebook,&nbsp;Twitter, and&nbsp;LinkedIn.
We are looking for self-motivated&nbsp;senior software engineers&nbsp;to join our core team developing Big Data technology for our massively parallel analytic platform. You must have skills including Java, C/C++, SQL, and scripting along with a strong desire to participate in the democratization of Big Data.
You can expect to&nbsp;learn&nbsp;or&nbsp;further develop&nbsp;key skills including:
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Parallel connectivity between Big Data repositories
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Automatic optimization of parallel connections
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Monitoring database and data movement
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Moving and transforming data in-flight
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Intelligent distribution and selection of data
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Predicate push down
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Parallel distribution of code, data, and configuration
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Working with other Big Data technologies such as Hadoop and Redshift
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cloud deployment and configuration
Primary Responsibilities:&nbsp;&nbsp;&nbsp;&nbsp; Design, develop, test, and maintain analytic platform features such as:
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; High speed, intelligent parallel connectors to Hadoop and other data sources
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Automated cluster generation, distribution and configuration
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cloud-based database management
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Both high and low-level analytic platform APIs
-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Interfacing with other systems such as SAS, R and Redshift
Required Skills and Experience

 Demonstrated ability to develop customer-facing technologies
 5+ years solid experience in C++ and/or&nbsp; Java
 Working knowledge of scripting languages such as Python, Perl, or Bash
 Working knowledge of SQL and database systems
 Capable of applying technical abilities to business needs
 Able to effectively communicate technology issues both within and outside of the Engineering department
 Experienced in object-oriented design and programming
 Linux platform (Redhat, CentOS, etc)

Other helpful skills:

 Prior Big Data experience such as Hadoop
 Understanding of database cluster management
 Work with storage hardware and software
 Linux tuning and management



Title: Big Data/Hadoop Architect with Consulting background experienceLocation: Santa Clara Duration: 6 -12 MonthsRate: Hourly
•To architect and design Large Scale Hadoop based solution for solving critical use cases related to analytics and insights leveraging Big Data.
•Lead the Big Data Architecture and Solution
•Implementation of Customer Sentiment Analysis using Transaction Logs, Customer Churn Analytics, Total Borrrow Risk Exposure solution, Network Fault Predictions and Web Log Analytics.
•Responsible for Big Data Product Ideation, Conceptualization,Feature Definitions,Architecture definition and Product Development for BigDataEdge
•Implement solution based on Text Analytics leveraging Hadoop for delivering insights from the Social Media and Websites for marketing team
•Implement Real Time Stream Event processing for large scale event co-relation for Auto-Scaling Engine
•Develop end to end Big Data Processing pipeline solution leverage the entire Hadoop ecosystem components (AVRO, WebHCat, PIG, HIVE and Oozie etc.)
•Implement end to end Business Transformation initiative for a large financial services institution
•Develop solution for SLA driven PAAS Management platform
•Lead the architecture definition and conceptual solution implementation for the next generation cloud management platform for a PAAS platform.
&nbsp;


Please send your resume to bdutta @ buxtonconsulting . com
Requirements:

 3+ years development experience in Hadoop.
 Proven expertise in Hadoop production software development.
 Successful track record of providing production support for large-scale distributed systems, with experience in creating software/scripts to automate production systems with some of the followings: Java, bash, python, etc.
 Must have atleast 1 year experience supporting production issues and problems.
 4+ years of experience programming in Java.
 Experience resolving complex search issues in and around the Lucene/Solr ecosystem.
 Experience designing and developing code, scripts and data pipelines that leverage structured and unstructured data integrated from multiple sources - TIFF/PDF, relational, web crawl.
 Must have experience in building analytics for structured and unstructured data and managing large data ingestion using technologies like Kafka /Flume/Avro/Thrift /Sqoop.
 Must have hands on experience in implementing solutions using MR, Hive, Pig, or HBase.
 Agile development methodologies.
 Must have a disciplined, methodical, and minimalist approach to designing and building software

&nbsp;
Responsibilities:

 Design and implement an integrated Big Data platform and analytics solution.
 Design and implement data collectors to collect and transport data to the Big Data Platform.
 Implement monitoring solution(s) for the Big Data platform to monitor health on the infrastructure.
 Collaborate with other teams to ensure on time delivery of scalable architecture.
 Adhere to development best practices including test driven development, and agile methodologies.
 Contribute ideas for continually improving the team’s productivity and code quality.

&nbsp;


The best way to apply for this job is to click apply button. We will respond to the qualified candidates. Thanks Texara Solutions.
&nbsp;

 We are looking for Sr 
 Hadoop platform engineers across USA.


 &nbsp;


 Assignments usually includes following tasks:


 
  Installing and stabilizing Hortonworks HDP based Hadoop environments or&nbsp;
  Upgrading HDP environments or&nbsp;
  Hbase installations and upgrades
 
 
  &nbsp;
 
 
  &nbsp;
 
 
  Positions are across USA. We have huge pipeline from various clients and we are estimating 70 similar assignments will have to be delivered in this quarter.
 
 
  &nbsp;
 
 
  This is great opportunity to experience various kind complex 
  hadoop big data architectures and environments.&nbsp;
 



Xoom Corporation&nbsp;(Nasdaq: XOOM), one of the fastest growing digital money transfer companies in the world&nbsp;is revolutionizing the international money transfer market by providing people with an easy, convenient and cost-effective means of sending money worldwide. We offer a secure, fast and inexpensive means of sending money from our website to both online and offline recipients in over 30 countries around the world.&nbsp;We are looking for a Data Platform Engineer. &nbsp;
In this versatile role, you will be responsible for helping shape the future of our mission critical NoSQL datastores.&nbsp; You will be collaborating closely with bright and knowledgeable engineering, product, and IT stars to design and manage a highly complex and heavily distributed environment. &nbsp; &nbsp;&nbsp;
Xoom is also an open source contributor and our engineers blog about our technology. Check us out at the links below!
http://dev-blog.xoom.com/
https://github.com/xoom
&nbsp;
Qualifications:

 7+ years of experience in Operational / DevOps role with Linux production systems in a 24x7 environment
 3+ years of NoSQL data modeling experience
 Must have hands-on noSQL experience from evaluation of new frameworks to deployment, maintenance, and performance tuning of clusters with at least 2 digit size
 Proficient with Cassandra, Hadoop, Elastic Search &amp; Redis
 Solid understanding of noSQL landscape, available frameworks, and CAP theorem
 Strong knowledge of how noSQL technologies interact and complement each other but also their limits&nbsp;
 Deep understanding of internals of noSQL frameworks
 Ability to build any noSQL open source framework from source
 Proficiency in one or more scripting languages - Perl, Python, Shell
 Strong SQL skills - Experience with MySQL
 Good prioritizing and time management skills
 Does not require training nor guidance to evaluate new frameworks and easily comes up to speed as self starter
 Programming experience in at least one non scripting language
 JVM performance tuning experience
 Strong in Linux and network maintenance
 Does not shy away to interact with open source community. Especially important when dealing with beta and alpha versions.
 Willing to take on challenging tasks under pressure
 Provide 24x7 on-call support in rotation

Desired Skills:

 Experience with regulatory compliance
 Background in statistics, quantitative data analysis or data-mining
 Open source committer or actively involved in community

&nbsp; Education:

 BS in Computer Science or equivalent technical training and professional work experience

Sound interesting? Apply now and we will contact you!


***Position can be located in most major US cities***
***Must have a Bachelors Degree***
***Must be open to 100% travel during the week, always home on weekends***
***This is a permanent full-time role, and we are unable to provide sponsorship***
Prestigious Global Professional Services Firm is currently seeking Hadoop Developers. Candidate&nbsp;define strategies, develop and deliver solutions that enable the collection, processing and management of information from one or more sources, and the subsequent delivery of information to audiences in support of key business processes. Candidate will adapt existing methods and procedures to create possible alternative solutions to&nbsp;moderately&nbsp;complex&nbsp;problems.&nbsp;&nbsp; Understands the strategic direction&nbsp;set by senior management as it relates to team goals.&nbsp; Uses&nbsp;considerable judgment&nbsp;to determine solution and&nbsp;seeks guidance&nbsp;on&nbsp;complex problems.&nbsp;&nbsp; Primary upward interaction is with&nbsp;direct supervisor. May interact with peers and/or management levels at a client.
&nbsp;
Qualifications:

 Minimum 1 year of building and coding applications using Hadoop components - HDFS, Hbase, Hive, Sqoop, Flume etc
 Minimum 1 year of coding Java MapReduce, Python, Pig programming, Hadoop Streaming, HiveQL
 Minimum of a Bachelor’s Degree

&nbsp;
Preferred Skills:

 Full life cycle Development
 Industry experience (financial services, resources, healthcare, government, products, communications, high tech)
 Minimum 1 year implementing relational data models
 Minimum 1 year understanding of traditional ETL tools &amp; RDBMS
 Experience leading teams

&nbsp;


Hi,
Please find the below and send resumes to raj@cyture.com or call me on 703-774-3772 asap.
&nbsp;
Role:&nbsp;Java Developer with Hadoop Knowledge&nbsp;
Location:&nbsp;San Francisco, CA&nbsp;
Duration12&nbsp;+&nbsp;months&nbsp;&nbsp;&nbsp;&nbsp;
Backend Java Developer with Java Map Reduce exp is MUST&nbsp;&nbsp;
Required skills:&nbsp;
&nbsp;- Strong java development skills and experience
&nbsp;- Debugging and analytics skills
&nbsp;- Knowledge of Hadoop and related components
&nbsp;- At least 1 year of Java Map Reduce experience
&nbsp;- Knowledge and experience in Databases, SQL and NoSQL.
&nbsp;- Knowledge in Spring, J2EE, REST and web services.&nbsp;
Top Three Skills needed to perform this role/job?
Core Object oriented and Java programming skills
Knowledge of Hadoop and Map reduce programming
Spring, J2EE and REST knowledge&nbsp;
Send resumes with:&nbsp;
FULL&nbsp;NAME:
POSITION #:&nbsp;Java Developer with Hadoop
YOUR LOCATION (City/State):
WILLING TO RELOCATE Seattle, WA: Yes/No
WILLING TO WORK 5 DAY WORK WEEK: Yes/No&nbsp;
PHONE:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
EMAIL:
SKYPE ID:
ALL INCLUSIVE RATE: $&nbsp;
RESIDENCY STATUS H-1B/EAD/GC/USC:
H-1B/EAD-OPT Expiration Date:
AVAILABILITY:


Senior Hadoop Reliability Engineer
Location: Redwood City, CA
&nbsp;
*Supporting Global VP of Technology to fill this role!*
** Contract-to-Hire OR Direct Full-Time Employment**
Our client's infrastructure has grown 8xto 40PB in last one year and it is still rapidly growing!!! We are looking for a rock star individual to join the team in helping us continue to &nbsp;build, operate, tune and scale our Big Data Infrastructure.
&nbsp;

 Routine stuff like general Linux system administration: configuration, installs, automation, monitoring, etc.
 Design, build, and at-times evaluate third party tools to manage a rapidly growing Infrastructure
 Automate operation, installation and monitoring of Hadoop ecosystem components in our open source infrastructure stack; specifically: HDFS, Map/Reduce, Yarn, HBase , Oozie, Hive, Tez, Kafka, Storm.
 Work with the team in providing hardware architectural guidance, planning and estimating cluster capacity, and creating roadmaps for Hadoop cluster deployment
 Continuous evaluation of Hadoop infrastructure requirements and design/deploy &nbsp;solutions (high availability, big data clusters, etc.)
 Troubleshoot and debug Hadoop ecosystem run-time issues.
 Recovering from node failures and troubleshooting common Hadoop cluster issues
 Document all production scenarios, issues and resolutions.
 Participate in a periodic on-call rotation.

&nbsp;
Skills and Experience:

 4+ years of engineering operations experience, including 1+ years of Hadoop experience.
 Strong Linux administration experience (
  
   7+ years, rock star level)&nbsp;
  
 
  
   1 year or more of 
   Hadoop experience from a large or small operation.
  

&nbsp;
Must be able to pass a background check &amp; drug test
&nbsp;



 
  
   &nbsp;&nbsp;Job Title: Java Hadoop Developer Primary Skills: Java, Hadoop, Cassandra, Hive &nbsp;&nbsp;Location: Oakland, CA Duration: 3 months CTH No. of Positions: 1 Position Type: &nbsp;Job Description: Job Requirement:Need someone ONLY FROM EAST BAY, CA(Fremont, Pleasanton) 10 years or more experience in Java Scala Play Cassandra Hive and Hadoop. Real hands on experience is needed because we will ask the person to write code during the interview process Must be 100% hands on. Apache spark is great to have. Good verbal and written communication skills. Previous startup experience required. Must be able to join full time after contract period. Will be based out of Oakland. Notes:&nbsp;&nbsp;&nbsp;&nbsp;
  
 



Job : Hadoop Developer
Location : Foster City, CA
Duration : 6+months
Looking for a talented Hadoop Engineer with experience on working with very large data sets and knowledge of building programs that leverage Hadoop and MPP platforms. The engineer will have significant knowledge of Big Data technologies and tools with the ability to share ideas among a collaborative team. Some of the responsibilities include loading data from several disparate, structured and non-structured data sets,documentation, performance testing and debugging applications.
The Hadoop Developer will also be an expert with traditional Database Development (SQL, Stored Procedures, User defined functions and ETL Development). The developer will understand Fact Dimensional Modeling and ETL steps necessary to load into Data Warehouse systems.
&nbsp;
Required
• Deep understanding and experience with Hadoop internals (MapReduce (YARN), HDFS), Streaming, Pig, HCatalog, Oozie, Hive, HBase.
• Ability to create and manage big data pipeline, including Pig/MapReduce jobs
• Deep understanding and experience with Linux internals, virtual machines, and open source tools/platforms
• Experience building large-scale distributed applications and services
• Fluent in at least one scripting language (Shell/Perl/Python/Java/etc.)
• Experience with agile development methodologies
• Knowledge of industry standards and trends
• Significant experience with Data Warehousing (Fact and Dimensional modeling) and RDBMS ETL development
• Expert SQL Developer is a must.
Please send resumes to gkrishna@us-buxton.com


 
 Experience in Hadoop Technologies - HDFS, Map Reduce, Hive, Sqoop, Zookeeper, H-base, Flume, Cloudera manager . 
 Good working knowledge of Pig Scripting, Oozie workflow and HBASE. 
 Define and develop client specific best practices around data management within a Hadoop environment. 
 Hands on experience on Core Java in Linux environment. 
 Work with functional team/ Frameworks team to understand requirements and translate them to technical specifications. 
 Cloudera certified is preferred. 



WE are looking for Strong ETL developers for our client in Sanjose,ca.
&nbsp;
ETL Developer with excellent hands on exp on informatica, Teradata &amp; PL/SQL,Hadoop is a must.
&nbsp;
Data Virtualization tools ( IBM Data Fedarator , Composite etc..)
&nbsp;
&nbsp;
Please forward resumes to uma@srsconsultinginc.com or call me @ 510-449-6440


Our client is looking for a java developer who has strong experience in Java technologies such as Core Java, J2EE, Struts, Spring, MVC architecture, JSP, etc.and our client give you an opportunity to learn Hadoop/Big data on the job
&nbsp;
Please send resumes to gkrishna @ us-buxton.com for immediate interview 


Looking for a talented Sr.ETL Developer/Analyst with loading data from several disparate, structured and non-structured data sets,documentation, performance testing and debugging applications.
The ETL Developer/Analyst will also be an expert with traditional Database Development (SQL, Stored Procedures, User defined functions and ETL Development). The developer will understand Fact Dimensional Modeling and ETL steps necessary to load into Data Warehouse systems.
Required
• Deep understanding and experience with Linux internals, virtual machines, and open source tools/platforms
• Experience building large-scale distributed applications and services
• Fluent in at least one scripting language (Shell/Perl/Python/Java/etc.)
• Experience with agile development methodologies
• Knowledge of industry standards and trends
• Significant experience with Data Warehousing (Fact and Dimensional modeling) and RDBMS ETL development
• Expert SQL Developer is a must.

 BIGData /hadoop is a big plus

&nbsp;Please send resumes to gkrishna @ us-buxton.com 


Greetings from Planetpro,&nbsp;
We are looking for a Hadoop Administrator with our direct client, I would like to discuss with you regarding this opportunity, Kindly go through the job description and let me know if you are comfortable with this opportunity so that we can discuss further.&nbsp;I can be reached on 925-307-7192 or akumar@planetpro.com&nbsp;
Hadoop Administrator
Full Time /&nbsp;Long Term Contract
San Mateo, CA
&nbsp;
JOB DESCRIPTION:
We are looking for a Hadoop Administrator to build, configure, deploy and tune Hadoop clusters. This individual will deploy and administer the Hadoop software, and ensure that the Hadoop infrastructure is implemented using best practice taking into account our technology stack, which this candidate will drive.
The Hadoop Engineer will work closely with the big data users, developers and server admin teams.
The ideal candidate will ensure that: systems function within defined SLA, upgrades and enhancements are non-disruptive and appropriate future planning is performed and acted upon.&nbsp;
QUALIFICATIONS:

 The candidate will be responsible for design and deployment of Hadoop cluster environments that can scale.
 Install and configure monitoring tools.&nbsp;
 Perform administrative functions for maintaining Linux servers’ hardware and operating system.
 Create and maintain system run books. Create and publish various production metrics including system performance and reliability information to systems owners and management.
 Proficiency with source control, continuous integration and testing methods (tfs,git).
 Experience in creating scripts/processes to automate production systems. Prior experience in lean/agile development environments.

Added skills (Nice to have):
Familiarity with Continuous Build Systems such as Jenkins. Experience with protocols or Fraud and Cyber security issues in either the Financial services or Telecoms sectors is a plus. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;
Thanks
Anish Kumar
akumar@planetpro.com&nbsp;
925-307-7192


Our client is looking for a java developer who has strong experience in Java technologies such as Core Java, J2EE, Struts, Spring, MVC architecture, JSP, etc.and our client give you an opportunity to learn Hadoop/Big data on the job
&nbsp;
Please send resumes to gkrishna @ us-buxton.com for immediate interview 


&nbsp;
Our client is a news making, technology company and they are looking for a&nbsp;Java Hadoop&nbsp;Developer. This is 12 months in San Ramon, and could go contract to hire. We can hire asap, so reply today.
&nbsp;
Experience in applying common design patterns, ability to communicate design ideas effectively using UML.

 Strong problem solving skills along with excellent verbal and written communication skills
 Expert understanding of multi-threaded priority based systems
 Defined and built n-tier Application Architectures that scale&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 Leadership and mentoring skills with an eye for reuse&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 Deliver world-class usability and user interface in our products &middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 Experience with Test Driven Development (Unit, Integration, Functional, CI) &nbsp;&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 Experience developing scalable back-end applications (Spring, Services, JMS)&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 Experience with Big Data technologies (Hadoop, NoSQL, HBase, Cassandra, MongoDB, MapReduce, Pig, Hive)


 Experience with Big Data Ingestion, Streaming and Analytics
 Experience with SpringXD, Pivotal Hawq, Pivotal Gemfire
  
   Experience developing web applications (Spring MVC, RESTful, SOAP, Jersey, JAX-RS, JAX-WS, Struts&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Experience with front-end technologies (JavaScript, HTML5, CSS, jQuery, AngularJS, BackboneJS, EmberJS, KnockoutJS)
  


 Experience with security/access control, scalability, high availability, online diagnoses, deployment, upgrade/migration, internationalization, production support and other SaaS enterprise software issues.

&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Experience with modern tooling (Gradle, Maven, Git, SVN)&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Experience with performance testing tools (SoapUI, LoadRunner)&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Experience with automation scripting (Python, Bash)&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Experience with database tools (JPA, Hibernate, TopLink, JDBC)&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;Experience with rules engines (Camel, Drools, Quartz, JRules)&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Demonstrated ability to excel in an Agile development environment (Scrum, XP, TDD)&nbsp;
&nbsp;


This Big Data and Analytics software company is a very hot startup in the mobile advertising business. &nbsp;Why is this compelling?It was funded recently in its &quot;A-round&quot; by Sequoia and Kleiner Perkins. &nbsp;The company has 35 employees and is very rapidly growing, especially on the customer / revenue side. &nbsp;Analytics software techniques are used to accurately identify individual users as they search the internet through different access devices (IE. desktop, mobile, tablet, etc). &nbsp;The ability to identify users accurately helps build a more comprehensive user profile which leads to much greater targeting abilities for advertisers and publishers. &nbsp;This is especially valuable with respect to mobile users who are historically very tough to target for advertising purposes.What you'll be doing 
 
  
  Hands-on with every aspect of our technical operations 
  Improve and scale a highly available ad serving platform to handle billions of requests daily worldwide 
  Implement infrastructure, system, application level monitoring to ensure high system availability 
  Improve and manage application/configuration deployment tools 
  Architecture design and capacity planning using system metrics 
  Work with a world-class engineering team to tune and scale our production environment 
  Design and see your creative ideas implemented in production 
  Automation, automation, and more automation 
  Share 24x7 on-call duties to maintain system availability - carry a pager every ten to twelve weeks after hours. 
  
 &nbsp;What's in it for you 
  
  Work in an exciting, evolving environment on the forefront of various data analytics and mobile solutions 
  Solving really challenging technical issues 
  Competitive pay, benefits and pre-IPO stock options 
  Work with the best and the brightest to promote and execute on exciting technology solutions 
  
 Requirements 
  
   
   Experience in highly available 24x7 production environment 
   Passion for automation - demonstrated some form of scripting (Perl, Python, PHP, or Shell) 
   Experience with Linux (ideally CentOS) and Apache 
   Experience with one or more of: &nbsp;Nagios, Monit, Cacti, Graphite, Redis&nbsp; 
   Direct experience with switch and router configuration as well as network topologies 
   Be an excellent and creative problem solver. You don’t need to know everything but you need to know how to find the solution 
   Self-motivated, organized and able to work independently in a fast-paced environment 
   Willingness to teach, to learn and be a generalist. This is a startup afterall 
   Excellent oral/written communication and documentation skills 
   Willingness to travel to colocation and occasional physical equipment install 
   
  &nbsp; 
   
   Ideal requirements: 
   experience with Puppet, Fabric, MySQL, Memcache, HAProxy 
   Experience with using Amazon Web Services or other cloud-based infr, including API and CLI 
   Experience with administration of Hadoop, HBase, Hive, and other open source big data technologies 
   Experience with Security 
   Experience with server virtualization technology, KVM, XEN, VMWare 
   
  
  



Our CompanyInformatica Corporation is the world’s number one independent provider of data integration software. Thousands of enterprises worldwide depend on Informatica data integration, data quality, and big data solutions to access, integrate, and trust their information assets residing on premise and in the Cloud.Our TeamThe Core Technology Group (CTG) is the innovation hub within Informatica that is responsible for delivering an extensible, reliable and scalable platform that provides the solid foundation for all other Informatica products. Specifically our team focusses on building a high performance and scalable Data Engine that can scale on multi-processor environments as well on a cluster of nodes. The engine is capable of handling batch, request/response and real-time workloads to deliver data for applications. Here are some of the areas you will be innovating as part of our team:•Cross compiler to translate representation of a data flow plan from one engine to another such as Hadoop•Reducing latency and processing time of query operations to return instantaneous results•Delivering large data sets to client applications with extremely high throughput&nbsp;•Enable the Data Engine to be available across on-premise, cloud, Hadoop and embedded applications.Your OpportunityThe Data Engine has been a key constituent of Informatica’s platform and serves multiple products such as Data Services, Data Quality and Profiling. Informatica products rely on the Data Engine to fulfill the vision of a Virtual Data Machine which is capable of executing data integration jobs either natively or across execution environments such as relational databases, Hadoop or cloud. You will be a key contributor to Informatica’s next generation platform and how it handles the volume, velocity and variety of structured and unstructured data. It could be enterprise and partner data from hosted Cloud services and on-premise deployments or arbitrary data from social web sites and media.&nbsp;Our Ideal CandidateYou are an experienced software engineer who has worked on developing large scale data management or database products. You relish the opportunity to work on building optimizations that improve data processing performance across different use cases that include batch, request/response and real-time systems.Your ResponsibilitiesThe successful candidate will be based in Redwood City, CA and will be responsible for:•Facilitating the collaboration and engagement between senior members of the team to ensure sound design and implementation of big functional areas for the metadata product•Incubating and designing the next generation platform for complex heterogeneous systems and applications involving cutting edge technologies•Working closely with architects and development and QA engineers to discuss and evangelize features•Collaborating with geographically dispersed, cross-functional teams in an Agile environment•Participating in internal/cross team meetings, scoping, decision making and technical documentation•Mentoring junior engineers on technical, architectural, design and related issues•Proactively identifying architectural weaknesses and recommending appropriate solutions&nbsp;•Taking architectural ownership of functional areas in the productRepresentative accountabilities may include, but are not limited to:•Leading and driving the delivery and architecture for significantly large functional areas in the product•Taking ownership for features in the platform that cut across multiple product groups&nbsp;•Designing, implementing, documenting, analyzing and operationalizing platform features clearly to ensure that others may readily utilize them•Interacting with QA and documentation teams to clearly communicate the features and behaviors of the system•Reviewing functional and test specifications and conducting training sessions for global services and support teams•Assisting customers and field engineers with troubleshooting issues and problemsYour Qualifications•7 or more years of relevant professional experience, a portion of which was within an enterprise software company•Knowledge of database internals such as query optimizations, developing core scalable parts of a system or server-side multi-threaded development experience is essential•Demonstrated ability to write quality code in Java and/or C++•Knowledge of query optimizations for distributed computing technologies such as Hadoop is a plus•Knowledge of NoSQL database management systems such as Cassandra and MongoDB is desired•Working knowledge of the REST architecture and principles of RESTful applications•Proven ability to work well with others in a fast paced, iterative product definition and development environment•Ability to communicate and express thoughts and ideas in a big group•Strong interpersonal and relationship building skills within an organization•Able to work independently with little direct supervision and take initiative; willing to mentor and develop others•Strong analytical problem solving and decision making skills•BS in Computer Science or a related technical discipline; advanced degree preferredInformatica offers a competitive compensation package that includes base salary, medical, retirement and employee stock purchase (ESP) programs, flexible time off and more. Our generous benefits vary depending on your geographic work location. It’s an exciting time to work at Informatica. You can learn more about our company, our products and our services at www.informatica.com. We are an Equal Opportunity Employer (EOE).


“Healing humanity through science and compassion, one patient at a time.”
Stanford Health Care
“We live in a time of unprecedented possibilities for human health. Bioinformatics, genomics, and other emerging disciplines promise to transform the very concept of medicine – from treating disease after it has struck, to predicting it, preventing it, and promoting lifelong health. The new Stanford Hospital will make this bold vision of personalized medicine a reality. It will empower us to deliver compassionate, coordinated, leading edge care, tailored to the unique needs of every patient. It will capture the promises of the biomedical revolution, translating the innovations of Stanford University and Silicon Valley into better health outcomes. A model of what health care can and should be in the 21st century, it will serve our community, and the world, for many decades to come.”
Technology isn’t just an enabler; it’s a leader in making the vision and mission of Stanford Health Care a reality. We have a unique opportunity to take amazing ideas and make them real with the organizational shift in IT delivery, with the partnership with IT innovators across the world, with the creation of a health plan, with the expansion of the Stanford Health Care network and the building of a new hospital that will redefine health care.
The Information Technology Services (ITS) group has invested a cross organizational capability in the creation of an Advanced Technology team whose charter, in partnership with key owners, is to:

 Continuously conduct market sensing on all ITS areas
 Evaluate and Compare a variety of offerings
 Develop and Demo proof of concepts/technologies
 Explore use cases in the utilization of the technology and assist with business case development
 Partner with owners to articulate their vision, create roadmaps and enable them to test it and readjust, if needed
 Push the traditional or current way of working with advanced technologies

&nbsp;Advanced Technology Architect
The Advanced Technology Architect is a visionary and strategist in technologies leveraged in the support and delivery of world class healthcare. After gaining a foundational understanding of the environment, the architect will form strategy and roadmaps with portfolio/service/capability owners, and continually test the model. This architect will at the same time get very dirty with the technologies to configure, test, fail, evaluate, re-configure, re-test and create scalable solutions. The architect will sense, research, partner with suppliers/owners, collaborate to develop, compare, suggest, demonstrate POCs, understand use cases/ROI, challenge the status quo, and execute technical solution strategies. This role will maintain and optimize a lab that is used to demonstrate the technologies. The architect is responsible for the overall technical vision from concept to solution. This technologist will benchmark world class technology, healthcare, and industry best practices and stay ahead of the curve leveraging any/all avenues from the web, to conferences and site tours; we will be leaders in our space.
Core Responsibilities/Essential Functions

 Work with Empathy for our Customers
 Evaluate solutions from a customer perspective in partnership with portfolio/service owners
 Listen to our end customers and ensure we are solving the right problems for the right reasons.
 Understand enterprise objectives and align technology requirements
 Conduct rounding and drive cross functional/organizational forums to solution architecture.
 Succeed Together, Internally and with Partners
 Serve as a thought leader in IT and trusted source that moves people with innovative ideas.
 Deliver and partner to improve ITS posture in methods and ability to deliver services.
 Drive business case development and impact to the organization
 Builds and leverages relationship with key technology partners/vendors
 Be Accountable
 Manage and modernize the advanced technology lab (physical, virtual, interfaces, promotions, etc.)
 Deliver on and against many R&amp;D projects, in various stages
 Possess architectural oversight across technology domains
 Ensure completeness of documentation
 Develop our People
 Share learnings on market sensing, conferences attended, and technologies experimenting with
 Leverage your experience to provide feedback to internal ITS teams on how to improve our services, mentor.
 Guide and participate in technology governance
 Communicate our learnings, our projects via newsletters or forums.
 Continuous Learn and Improve
 Learn and demonstrate use groundbreaking technologies both relevant and experimental to SHC, take risks.
 Stay ahead of the consumer technology market and understand technology trends across domains
 Interact with leading technologists and healthcare providers around the world, grow in healthcare.
 Push the envelope and enable others to do the same.

Education:
Bachelor’s degree in Engineering, Computer Science, Information Technology or in a work-related discipline/field from an accredited university.
Experience: 
Ten (10) years of progressive responsibility in the design, development and support of technologies.
Ten (10) years possess breadth and depth of knowledge in data, applications, web, compute, platform, and mobile architectures.
Any combination of education and experience that would likely provide the required knowledge, skills and abilities as well as possession of any required licenses or certifications is qualifying
Knowledge, Skills, and Abilities

 Ability to plan strategically both long-term and short-term
 Ability to lead and communicate effectively at all levels of the organization, create buy-in
 Ability to apply information technology in a health care vertical/setting
 Ability to be user-focused, build relationships and communicate effectively
 Ability to plan and execute vendor and product selections
 Ability to partner with innovation teams to quickly assess new technologies, digest value, and prototype in the lab.
 Ability to demonstrate initiative, good judgment and effective decision-making; exhibit strong process improvement orientation, and have the ability to achieve results through other programs and individual
 Demonstrated cost reduction and cycle time improvement with technology solutions
 Ability to develop business cases for new technology solutions
 Knowledge of technology trends and ability to collaborate with multiple service providers
 Ability to work effectively and extract value from partners
 Ability handle multiple parallel activities
 Health Care provider organization or medical device experience preferred
 Experience working in an application area deployed within SHC a plus– Clinical (Epic), Imaging, Lab, Business &amp; Financial, etc. preferred
 Experience managing and optimizing a technology lab (physical/virtual/stages/customer experience) preferred
 Experience in building a rapid prototyping environment preferred
 Experience with cloud (computing, platform, storage, desktops, etc.) Zoning in public/private clouds preferred
 Experience with configuration automation with management tools like SCCM, Chef, Puppet, etc.
 Experience with monitoring tools like Solarwinds, Nagios, MRTG, Munin, Zenoss, etc.
 Experience with infrastructure (data center, networking (certifications a plus)), switches, load balancers, applications, servers, storage, etc)
 Experience with at least one scripting language: Powershell, Python, Ruby, Perl, or similar.
 Experience with at least one query language desired: mySQL, SQL, noSQL, MongoDB, or similar.
 Knowledge of software within the Hadoop ecosystem: HBase, MapReduce, Pig, Hive, Oozie, Flume, or similar.
 Proficiency with virtualization (ESXi, vCenter, Hyper-V, Desktop/DaaS) or similar.
 Awareness in concepts with end user devices or similar.
 Knowledge in Windows/Unix/Mac/IOS/Android OS, or similar.
 Experience with IT Service Management Tools like ITSM, ServiceNow and software distribution, or similar.
 Knowledge of IT best practice methodologies (Lean, Agile/DevOps, ITIL, EA, etc.)
 Awareness of FDA/HIPPA regulatory/security requirements
 Open to working non-standard hours (no night shifts) including a Sun-Mon or Fri-Sat weekend, if required
 Open to attending organizational, scrum/stand-up, and rounding meetings
 Open to representing team in forums and rounding with customers and stakeholders

All qualified applicants will receive consideration for employment without regard to race, sex, color, religion, national origin, protected veteran status, or on the basis of disability.
Interesting Links
Building of the new Stanford Hospital
http://medicalgiving.stanford.edu/sm-mcd/en/new-stanford-hospital.html
http://medicalgiving.stanford.edu/smsites/content/dam/sm-mcd/resources/hospital%20case%209-18-13jr-web.pdf
About Stanford Health Care
http://www.stanfordhospitalcareers.com/home/Who-We-Are/
http://www.stanfordhospitalcareers.com/home/Who-We-Are/Our-Awards
http://universityhealthcarealliance.org/
http://news.kron4.com/news/best-hospitals-ranked-for-2014/
http://www.bizjournals.com/sanfrancisco/blog/2014/05/valleycare-health-system-explores-stanford-merger.html?page=all
Other Interesting Links:
http://www.informationweek.com/healthcare/patient-tools/stanford-hospital-offers-doctor-visits-via-video/d/d-id/1111614?
http://www.himssanalytics.org/general/pr_20100122.asp
http://www.healthcatalyst.com/himss-2014-day-3-the-state-of-health-care-analytics/
&nbsp;


Position:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hadoop Developer 
Location:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; San Francisco, CA
Duration:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6 Months
Start Date: ASAP
Required:

 Advanced Linux administration Hadoop administration experience with a drive to develop expertise and understand the internals Experience with Chef Configuration management with a keen interest in developing expertise.
 Ability to provision new cluster nodes from bare metal to full cluster membership (refactoring build process as necessary).
 Basic knowledge of JVM tuning Basic ability to read Java code and interpret stack interfaces.

&nbsp;
Preferred skills:

 Familiarity with Hive data access patterns and basic MapReduce tuning.
 Familiarity with HBase, YARN, Kafka Experience using Sqoop Some capacity planning experience.
 Experience installing and working with monitoring tools like Nagios , ganglia.
 Understanding of distributed system concurrency, threading and parallel algorithm implementation.
 Familiarity with distributed computing concepts such as Paxos and ZAB protocols CAP theorem, vector clocks CALM principle, CRDT's etc.
 Must have strong English written, verbal and presentation skills

&nbsp;
Thanks &amp; Regards&nbsp;
Srikanth Rao| Talent Acquisition Manager
22260 Haggerty Rd, Suite 285, Northville, MI 48167
(&nbsp;&nbsp;248-270-3865 | F 248.715.6434)
srikanth.rao@rsrit.com
An E- Verified Company
Certified Minority Business Enterprise (MBE)
&nbsp;


We have a direct client opportunity for 
Java Software Architect role in Sunnyvale, CA.

If you or anyone in your network is interested in this role, please send me your updated profile to 
archana@mumbatech.com


Role: Java Software Architect


Location: Sunnyvale, CA


Duration: Full Time.

&nbsp;


 
 BS/MS CS/SE/EE degree or equivalent with 8+ years of Java/J2EE experience in the field of Software Engineering and Development
 
 Minimum 6+ years of strong hands-on development experience with Java (5.0/6.0), J2EE and related technologies
 
 Hands-on experience as Architect working very closely with Business Users.
 
 Working Knowledge on Reactive/Functional Programming is highly desirable
 
 Knowledge of NO-SQL technologies like MongoDB IS A PLUS.
 
 Strong understanding of design patterns and best practices in Java/J2EE platform including UI, Application &amp; Frameworks development
 
 Extensive hands-on development experience with frameworks and tools including Apache Stack, Web Services (SOAP/REST), WS-* stack, Hibernate, Maven etc.
 
 Must have prior experience in leading Technical deliverables. Must be able to effectively communicate &amp; work with fellow team members and other functional team members to coordinate &amp; meet deliverables
 
 Exceptional communication, organization and presentation skills
 
 Experience working with Open Source Frameworks is a plus
 
 Nice to have experience with Big Data &amp; NO-SQL related technologies (Eg: Hadoop, Cassandra etc.) IS A PLUS.
 


&nbsp;&nbsp;


IT Data Engineering - Data Analyst - Matlab, C++, Python, SQL (no Ph.D)
Work with multi-disciplinary teams in junction with partners.
Research and develop new methods for information extraction, knowledge representation, data manipulation, reasoning and modeling for application to a diverse set of problems.
Demonstrate a strong technical background and independently execute innovative projects in areas of data mining, statistical models, knowledge representation, information extraction and fusion, and data compression.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Build innovative computational models for enabling better analytics, data mining, and data manipulation technologies.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bridge novel semantics, domain knowledge, and mining technologies to provide meaningful analysis of large scale datasets.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Design and construct solutions to enterprise-wide problems for industrial, service and finance business problems.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Develop strategic plans, manages projects and guides teams to create innovative solutions.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Demonstrate breadth as well as depth in problem solving.
•&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;Acquire technical knowledge of technologies and proofs of concept.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Effectively communicate technical analyses and results to business management
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fluent oral and written communication in English.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Basic Qualifications:
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Master’s degree in Engineering, Mathematics, Applied Statistics, Computer Science or related discipline with at least 3 years of experience in developing analytics and data mining technologies.
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Experience with prototyping and developing data mining applications using statistical software packages (Matlab, R, etc.). Object oriented programming experience (Java, C++, etc.).
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Experience with big data computing concepts such as parallel computing, distributed computing and/or cloud computing, and related technologies such as Hadoop and/or other NoSQL technologies, Cassandra, Gleenplum and more
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Experience with Java and database technologies, such as relational, hierarchical and graph databases, distributed data file systems, data federation and query optimization
•&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Demonstrated ability to implement high quality analytics or software in diverse areas
&nbsp;
If interested please mail your resume at asen@netpace.com or call me at 805 298 0704
Thanks,


Estuate is a global information technology (IT) Services Company founded on the simple premise that our clients value a partner that is both technically proficient and highly responsive to their needs. Since our inception in 2005, we have been striving to stay true to that principle. We combine an agile, responsive U.S.-based culture, with a strong technical offshore team to deliver high-value, cost-effective solutions to software companies and large IT organizations. We are headquartered in Silicon Valley and maintain our offshore centers in Bangalore and Mangalore, India. Our executives honed their skills with decades of experience at Oracle in product development and client services, in both Oracle Applications and technology.
&nbsp;
Currently, we have an immediate requirement for a BigData Architect, per the details given below
&nbsp;
Location &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Sunnyvale, CA
Start &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Immediate
Duration &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6 months (extendable)
Job Details
&nbsp;
&nbsp;
Skills/Qualifications:
&nbsp;

 BS or MS degree in Computer Science or equivalent


 8+ years of experience in software and architecture design and development of large scale, fault tolerant and highly scalable enterprise and web based applications
 Solid experience in Core Java development and design and development of web services within large scale, fault tolerant, multi-threaded distributed systems.


 3+ years of experience in the development of Hadoop APIs and MapReduce jobs for large scale data processing.


 Strong background in Big Data technologies and the Hadoop ecosystem (i.e. Hive, Pig, Flume, Storm, etc.)
 Experience working with NoSQL data stores like HBase, Cassandra, MongoDB, etc. and RDBMS such as Oracle DB and SQL Server.
 Experience with BI, data analytics and MPP databases like Vertica.
 Familiarity with cloud services like AWS, Rackspace, or HP Cloud.
 Solid understanding and experience with extract, transform, load (ETL) methodologies in a multi-tiered stack, integrating with Big Data systems like Hadoop and Cassandra.
 Hands-on experience working in Linux, Unix, Windows environments.
 Must be a team player and enjoy working in a cooperative and collaborative team environment.
 Passionate on learning new technologies and standards
 Strong verbal and written communication skills
 Experience working in onsite/offshore model

Responsibilities:

 Architect, Design and develop web services for large scale, multi-threaded applications.
 Hands on technical role; Contribute to all phases of the SDLC, including but not limited to analysis, architecture, design, implementation, and testing.
 Developing TB per month scale web services predominantly in Java.
 Strong SQL and NoSQL data store usage: querying, tuning, mapping, and operations.
 Architecting, provisioning, tuning, and development of Hadoop or other distributed systems.
 Design, Prototype and drive Enterprise solutions across multiple business domains.
 Run technical forums across multiple business units and provide feedback and best practices.
 Formulating MapReduce ETL processes and working with downstream data warehouses for analytics.
 Responsible for architecture/design documentation and participate in design/code reviews
 Work with cross functional teams including onsite and offshore and mentor teams

&nbsp;
Please send us your resume to &lt;sharath@estuate.com&gt; to take this forward immediately
As this is a very urgent requirement, an immediate reply is highly appreciated
&nbsp;
&nbsp;
&nbsp;Thanks and Regards

 
  
   
    
     
      
       Sharath | Estuate, Inc.
      
      
       P : 408-625-7591
      
      
       P:&nbsp; 408-689-2602
      
      
       1183 Bordeaux Dr, Suite 22 Sunnyvale, California 94089
      
     
    
  
  
   
    
     
      
       &nbsp;&nbsp;WHITEPAPERS |CASE STUDIES | RECORDED WEBINARS
      
     
    
  
 

&nbsp;


Principal Software Architect
Duration: Full-Time
Location: Sunnyvale, CA
&nbsp;
Job Description:&nbsp;&nbsp; Global Software Engineering team at is seeking a hands-on architect to lead Big Data architecture and engineering team. Ideal candidate would have experience in NoSQL stores, distributed computing systems such as Hadoop, real time processing engines among other evolving big data technologies. Candidate must have experience and deep understanding of building robust and scalable systems designed for performance, reliability and scalability.
&nbsp;
Responsibilities:

 Design, architect, build and extend Big Data Platform to cater to data streams from 100 different data centers across the world.
 Provide thought leadership, strategy and lead innovation by exploring, investigating, recommending, benchmarking and implementing Big Data technologies.
 Own development tools, processes, quality/performance for development work.
 Work with world class engineering teams for integration with different systems, data center infrastructure and industrial control systems.
 Collaborate with network architects, engineers, analysts and product management to define roadmap.
 Reports critical issues to leadership team and management effectively, timely and with clarity.
 Hands-on system design and development.

&nbsp;
Qualifications &amp; Experience:

 10-15 years of experience in software development
 2+ years of hands-on experience with NoSQL technologies such as Cassandra, HBase and Hadoop eco-system (MapReduce, Pig, Hive, Sqoop etc.)
 Must have hands-on experience in real-time &amp; stream processing systems such as Storm, Spark
 Experience in architecting and building large scale systems requiring high availability and high scalability
 Knowledge of cloud computing infrastructure (e.g. Amazon Web Services EC2, Elastic MapReduce) and considerations for scalable and distributed systems
 Must have a deep understanding of different SDLC methodologies and experience with all phases of SDLC
 Highly proficient in Java/J2EE eco-system technologies, caching and persistence technologies
 Hands-on experience in using scripting technologies such as Python and Bash
 Experience &amp; knowledge of Apache Mahout, R language, statistical modeling, predictive modeling and machine learning is highly desirable
 Ability to effectively communicate across globally distributed cross-functional teams and roles
 A deep understanding of governance process for large scale, distributed and multi-tenant architectures
 Bachelor’s degree in Computer Science/Engineering or equivalent

&nbsp;
FOR IMMEDIATE CONSIDERATION - please contact diane@mumbatech.com and/or (408) 533-1056


Informatica ETL developer
Summary:
We are seeking an Informatica ETL and Quality technical development lead to help us establish and grow our data management practice. The perfect candidate would have experience working with the full data management Informatica suite. This role requires a person who can provide guidance to the Architectural and Development groups around how MDM should be technically implemented as well as maintained. They should be able to contribute to developing a framework of principles, standards, practices, and roadmaps aligned with business objectives.
Skills and Expertise:

 Strong understanding of the complete SDLC
 3-5 Years’ experience with Informatica Power Center and Data Quality
 3+ years of data warehouse development experience
 Exposure to data distribution, MDM and BPM technologies
 Extensive experience around connecting to a wide variety of
 Strong experience with web services and SOA technologies
 Strong experience integrating cloud solutions example - SalesForce and WorkDay
  
   Either with in-house systems or other cloud solutions
  
 Experience with the following technologies
  
   SAP ECC, BW, BPC, BA, HANA
   Teradata
   Hadoop
   Oracle, SQL Server
  
 In depth hands on experience and proficiency in
  
   PL/SQL scripting
   Unix and Perl Scripting
  
 Experience with large multi-terabyte to Petabyte environments.
 Experience with modeling tools like
  
   SAP Power Designer
  
 Strong problem solving and decision making in the areas of
  
   Solving problems quickly
  
 Good documentation and communications skills

Strong proven SAP ECC as well as BW Integration is a must.
Any data migration and HANA Integration experience is a plus.
&nbsp;
Roles and Responsibilities:

 Participate in the analysis, design, development and testing of the most complex of software components.
 Develop a solid understanding of the various sources that feed into the warehouse and assist in laying out the overall architecture of the ETL application.
 Create supporting documents that describe in details the overall ETL architecture of the application.
 Work on day to day development tasks in priority order as assigned by the Tech Lead.
 Develop, document and execute Unit Test Plans.
 Provide technical support during system testing of the application. Complete code migrations, manage versioning of ETL components, and manage changes with other in-flight projects.
 Contribute to implementation plan and other supporting implementation documents.
 Participate in estimation activity for development efforts.

&nbsp;
Regards
Harish
harish.rao@prospanceinc.com
Prospance Inc


Primary Function of Position:

We are seeking passionate software developers and architects to help pioneer and build an organization that enables all users and developers of our surgical robot to extract meaning from the profuse amount of data our robot can create.

To do this, we need to work with the embedded systems of the robot; model and store vast amounts of data; define and deploy server architecture; build aesthetically crafted intuitive tools; and more. 

A true full stack opportunity creating an extremely groundbreaking product that positively impacts the lives of people.

Roles and Responsibilities:

* Work on a cross-functional team to design and develop and deploy REST-ful API*s and web frameworks to interact with the da Vinci* Surgical System 
* Define, architect and deploy data hierarchies to deal with vast amounts of unstructured data
* Work with other team members to improve the quality and capabilities of our Python interface and our web-based application suite that is used to diagnose, troubleshoot and interact with the surgical system.
* Assist with the analysis and presentation of the abundance of data originating from our surgical robots. (error logs, events, kinematic data etc)
* Contribute to an environment that enables engineers, field service personnel, manufacturing and product support to extract, analyze and visualize information from the robot that in turns enables them to excel at their jobs and thus improve patient value of our products.

Skill/Job Requirements: 

We have a wide spectrum of work to do and are looking for the right person over a precise set of skills. The candidate will ideally have some combination of following skills/abilities:
* Educational background Math/Statistics, Software, Electrical, or Computer Engineering related program.
* Enthusiasm and passion for technology.
* Interest in data analytics and a desire to derive meaning from reams of data
* Proficient with the following:
o Python
o Web Development skills 
 HTML, CSS Javascript, jQuery, Flask, Django, AJAX, JSON
o Ability to understand and program C
o Database experience (MSSQL, MySQL, MongoDB)
* Nice to have experiences:
o Linux shell environment and administration. 
 Bash skills
 Nice to also have programming experience in the Linux (drivers/apps/kernel)
 Experience with cross compilers also a plus
o 
Hadoop, Hive, Big Table or other *big data* experience
* Strong ability to isolate and debug software problems
* A real excitement to learn and get to the bottom of tough technical problems


Greetings!!! While applying, please mention &quot; DD_HDA &quot; in the subject line. devang@intelliswift.com
&nbsp;
1. Strong SQL Experience (process, analyze data, troubleshoot log files. 2. Strong familiarity with Hadoop (understand how it works, and write a quick program) 3. Tableau plus (or any presentation tool)
&nbsp;
Primary Job Requirements• Performs complex data validations and analysis in support of ad-hoc and standing management or customer requests. • Develops programs, methodologies, and files for analyzing and presenting data. • Uses and supports database applications and analytical tools. • Uses timely and appropriate participation of users/customers in data collection, validation and analysis. • Provides accurate and appropriate interpretation of data, applying knowledge to evaluation, analysis, and interpretation of data. • Works with management and/or customers to develop and understand requirements. • Communicates regularly and effectively with team members and management. • Communicates results effectively to management and/or customers. • Ability to write and execute complicated SQL queries and visualizations (Tableau preferred). • Ability to manipulate and create data for testing through SQL, Hadoop-Pig. • Strong problem solving skills. • Ability to read and understand basic software code. • Ability to translate requirements and software design into robust test cases. • Experience with troubleshooting issues - interpreting stack traces, verifying data in logs, manipulating data, thinking outside the box to recreate issues, etc. Experience with Hadoop/Teradata based applications. 


Hi,
Our Client is looking to procure a qualified candidates for Big Data Architect Requirement at San Ramon, CA. Enclosed below is the Job Description for your reference. Kindly forward the resumes in word format, along with contact details for proceed..
Position Big Data ArchitectLocation San Ramon, CA ( Need Locals ONLY)Duration 6 Months+
Responsibilities: • Participate in system design and architecture decisions.• Demonstrate analytic and problem-solving skills, particularly as it relates to data integration, aggregation and processing• Experience working with Oracle databases and working with Data Architecture• Located in Bay Area, CA• Excellent communication and relationship skills• Ability to thrive in a fast-paced, customer-oriented environment.
Required Skills: • BS/MS in Computer Science or other relevant degree• 8+ years of experience with 2-3 years of practical experience large volume data processing• 5+ years of programming experience working with JAVA• 2-3 years Hadoop development experience
Nice to Have Skills • 1-2 year experience working in in-memory databases like Gigaspaces, Gemfire, SQLFire• Experience working with Apache Camel or other similar Rule and mediation engines• Experience with Complex Event Processing and stream processing technologies like Apache Storm.
&nbsp;
Thanks
Sudhir DubeyUnited Software Group Inc.. 565 Metro Place South. Suite # 110 Dublin, OH 43017 Phone: 614-588-8608 Fax: 1-866-764-1148 sudhir.dubey@usgrpinc.com www.usgrpinc.com
G-Talk/YM/Skype :skd.staffing/skd_staffing/ sd_professional


Role: Hadoop Developer
Location: Redwood City, CA
Start date: ASAP
Duration: 12+ months
Looking for a talented Hadoop Engineer with experience on working with very large data sets and knowledge of building programs that leverage Hadoop and MPP Database platforms. The engineer will have significant knowledge of Big Data technologies and tools with the ability to share ideas among a collaborative team. Some of the responsibilities include loading data from several disparate, structured and non-structured data sets, documentation, performance testing and debugging applications.
The Hadoop Developer will also be an expert with traditional Database Development (SQL, Stored Procedures, User defined functions and ETL Development). The developer will understand Fact Dimensional Modeling and ETL steps necessary to load into Data Warehouse systems.
&nbsp;
Required
•Hands-on Experience using MapReduce
•Hands-on Experience using projects in Apache Hadoop ecosystem such as FLUME, Pig, Hive, HBase
• Fluent in at least one scripting language (Shell/Perl/Python/Java/etc.)
vExpert SQL Developer preferred.
• Deep understanding and experience with Hadoop internals (MapReduce (YARN), HDFS), Streaming, HCatalog, Oozie
•Hands-on Experience in the following areas: Elasticssearch, LuceneSearch, JQuery
•Strong desire to work for a fast-paced, flexible environment of a startup
• Ability to create and manage big data pipeline, including Pig/MapReduce jobs
• Deep understanding and experience with Linux internals, virtual machines, and open source tools/platforms
• Experience building large-scale distributed applications and services
• Experience with agile development methodologies
• Knowledge of industry standards and trends
• Significant experience with Data Warehousing (Fact and Dimensional modeling) and RDBMS ETL development
&nbsp;
Qualifications:
•BS in Computer Science or a related field
•Java experience
&nbsp;Please send resumes to gkrishna@us-buxton.com 


Position: Unix Systems Engineer (with Middleware)
Location:&nbsp;San Francisco CA
Duration:&nbsp;6-18 months
&nbsp;
Description:
(*) Maintain and support all process requirements for OS upgrades, open source software registration, access requests, security plan updates, firewall requests (*) Create and keep current all Sharepoint documentation on lab management details for servers, projects, and knowledge base (*) Excellent communication skills, will be working with people in person, over the phone, or via IM/Email.
Experience supporting applications that are built on
* Unix/linux (any flavor works), Windows 2008 Server * Unix/linux (RedHat preferred experience), Windows 2008/2012 Server experience * Windows .NET and IIS experience, SQL Server 2008/2012 experience
The windows experience is lower priority, however, preferred, as we have some items on these platforms and are fully supporting them.
* Tomcat (any middleware works like weblogic/jboss etc …, if has tomcat that will be great) * Oracle (any database works, if has Oracle that will be great), MySQL, (other in memory DB's or Hadoop) * SVN (any version control is fine, if has SVN that will be great) * Knows how F5/ASM/Networking/SSL certs works * Autosys (any job scheduling tool is fine) /Shell scripting (any programing language is fine) * Must be very good on Process, good understanding of practical knowledge of SDLC (software development life cycle) * Previous WF technology experience, especially with internal systems, ART, SR, Pac2k, all must be certified
(*) Work ethic / approach to the role (*) Demonstrate the key ability to problem solve, trouble shoot and discover root cause of issue problems and take a preventative approach rather than reactive (*) Build and maintain relationships with all internal and external systems/teams/vendors (*) Fully owns and is accountable for all aspects of lab management, application setup, documentation - Understands the process flow - Physical infrastructure - Internal network routings - Multiple Data Center/Lab server implementation - Interfaces with all other internal systems - Interfaces with external partners/vendors
&nbsp;
Interested/qualified candidates please send your resumes with contact details, current location and visa status to :-KiranJobs@zumeit.com/kirand@zumeit.com248-522-6868 Ext :114



Candidate must have:&nbsp;
• 10 years industry experience developing Java based applications.&nbsp;
• Min of 2 years of experience and strong knowledge in Hadoop stack, no-sql or graph DB.&nbsp;
• Excellent working experience and a strong knowledge in Core Java, Multithreading, complex data structures&nbsp;
• Experience in solution designing&nbsp;
• Excellent analytical, problem solving and communication (written and verbal) skills&nbsp;
• A strong desire and aptitude for learning new technologies&nbsp;
• Ability to work in a fact paced, self-directed, action-oriented environment&nbsp;
• Experience in big data technologies – HDFS, M/R, HIVE, Cassandra, HBASE, Implala, Spark or other relevant technologies is preferred&nbsp;
• Any work related to new invention reports on patents filed in past is preferred.&nbsp;
• Any experience with functional programing language like Scala, Grovy etc for rapid prototyping is preferred.&nbsp;
• Experience &amp; knowledge in machine learning algorithms is also a plus
&nbsp;


We are currently seeking a&nbsp;Data Engineer&nbsp;to join the Business Intelligence team in a permanent role at our Emeryville, CA office to help build and maintain large scale Big Data and Relational Data Warehouse application infrastructure. &nbsp;We're looking for someone who is experienced in all parts of the Business Intelligence process, with an emphasis on ETL (we mostly use Python for scripting), Hadoop and RDMS experience. &nbsp;Visualization experience is desired but will be a very small aspect of this role and should not be the bulk of your experience.
&nbsp;
The Gracenote service is a unique, high performance, massively scalable Internet media recognition system tailored to the exploration of music and video. We process hundreds of millions of queries a day from millions of users across the globe, with an average response time in milliseconds. The service is the technology behind many of the most exciting desktop, mobile and consumer electronic products and apps in the marketplace today.&nbsp;
&nbsp;
You will be a key resource, in a small highly collaborative team, to build and support a wide range of internal data system that we are using to scale and maintain our applications.
&nbsp;
Responsibilities

 Develop ETL and Analytics on Gracenote’s data platform in Hadoop and RDMS.
 Build reliable, high availability data services and real time processing routines on very large &nbsp;data volume
 Work closely with stakeholders including&nbsp; product management, engineering, professional service and content operation teams to understand requirements and deliver analytical and dashboard functionalities
 Provide technical input on architecture and solution choices
 Provide scope and schedule estimates in order to support project and program planning
 Participate in code, design and deployment plan walkthroughs
 Keep updated on and recommend new technology alternatives and direction.&nbsp;
 Participate as Hadoop and database SME for IT Roadmap planning
 Help build and maintain our data pipeline and infrastructure
 Produce and maintain documentation to support maintenance requirements and code revisions
 Implement necessary processes to ensure SLA for system uptime is met or exceeded.
 Available for troubleshooting unplanned system impairments or outages, supporting escalation process

&nbsp;
Non-Technical Requirements

 Passionate about building great products using data and analytics
 Passionate about data technologies and data-driven development practice
 Do what it takes to get things done
 Willing to listen and work with users/partners to deliver the results
 Team player
 Willing to learn from others and share knowledge with others
 Understand and follow the SDLC, while being flexible, result and deliverable oriented
 Communicate well and effective in cross-functional environment

&nbsp;
Technical Requirements

 Demonstrated ability to deal with very large datasets in Hadoop or RDBMS environment.
 Hands on in using relational database technologies in highly concurrent applications
 Hands on and must be able to code and deliver ETL and Analytics solutions apart from architecture and mentoring
 Strong in SQL and Store Procedures
 Strong programming skills preferably in Python
 Highly analytical with a structured way of thinking
 Thorough understanding of the Hadoop ecosystem of products like HBase, Impala, Hive, Solr (on HDFS), Spark
 Recent experience working with Hadoop-based production implementations.
 Performed performance tests to achieve benchmarks against SLAs
 Hardening Hadoop clusters for deployments into production environments
 Troubleshooting experience with common Hadoop cluster issues
 Supporting Hadoop developers and assisting in troubleshooting and optimization of Hadoop supported applications.
 Computer Science, Math related academic background

&nbsp;
Desired Qualifications and Skills

 Experience&nbsp; installing, configuring, monitoring, and maintaining HDFS, HBase
 Database engineering and administration experience in Linux platforms&nbsp;
 Database performance tuning and troubleshooting (Master/slave, Load&nbsp;balancing)&nbsp;
 Relational database design and Linux system administration&nbsp;
 Experience of Storm, Spark, Kafka, graph databases and NoSQL database would be beneficial
 Experience with code repositories (i.e GIT)



Minimum Required Skills:

Hadoop, HBase, Java

We are an early data security analytics startup in Palo Alto, CA, and we are growing. Our company consists of fifteen people in total and we are looking to grow to 40 by the end of the year. We are backed by top-tier venture capital firms, and led by experienced management team and visionary entrepreneurs. The founding team is a rare mix of interdisciplinary experts in cybersecurity, big data, data science and seasoned entrepreneurs all coming together at the right time to meet a large and immediate market need in enterprise security space. Our organization is run flat and everybody in contributes directly to the product. We collaborate, scale, and look for ways to make the digital world more secure.



What we are looking for:



We are looking for a Senior Big Data Software Engineer with strong experience in building end to end backend stack that uses distributed architecture and deals with large amounts of data with numerous access patterns (search, analytics, aggregations). You need to be comfortable working in a startup environment and building version 1.0 products from scratch. You need to have working knowledge with 
Hadoop/NoSQL platforms and get excited about developing innovative software applications on big data platforms. You need to be comfortable working with cutting edge UX/UI developers to make the product a snap to use while interfacing with latest in big data technologies and data science.

What You Will Be Doing

You will be a key member of the product development team that helps shape the capabilities of the product.

You will have responsibility of the design and development of our backend platform that that supports large-scale data ingestion, feature extraction, model development and scoring systems built on 
Hadoop and related technologies.

You will migrate prototype models into scalable enterprise grade software with distributed computing and optimization techniques.

You will define and develop APIs for integration with other components of the product.

What You Need for this Position

Solid experience in software development, preferably in distributed computing environments or big data platforms.

Expertise and experience in one or more of the following languages: Java, Python, C/C++, Pig, and Scala.

Hands-on experience with 
Hadoop and MapReduce programming.

Experience with Maven, Jenkins, Git.

Good understanding and working knowledge of SQL and NoSQL databases.

Advanced knowledge of API's, JSON, BI concepts and processes (e.g. ETL, data aggregation and analysis) a must.

Solid understanding of TDD, where scripted unit tests are provided as part of software hand-off criteria, is required.

Experience developing intrusion detection systems, SIEM or next generation firewalls is desirable.

Understanding of web protocols including HTTP a plus.

Experience with search systems (e.g. Solr, ElasticSearch) desirable.

Experience working in an Agile environment



Some of the technologies we use: 


Hadoop 

HBase

Pig &amp; Hive, Scala

Solr/ElasticSearch

Java/JavaScript/Python

D3/Node.js/Angular.js

JSON/REST

AWS (EC2, S3, RDS etc.,)

What's In It for You

As an early employee you will get a true stake in the company. We will offer a very generous equity play.

Competitive salary: 150-175K

Vacation days &amp; time off

Awesome medical, dental, &amp; vision insurance

Company dinners &amp; happy hour

Fully stocked kitchen with snacks and beverages

Great location in Palo Alto downtown with cafes, restaurants, public transportation within walking distance.So, if you are a Big Data Engineer with strong experience, please apply today!

Applicants must be authorized to work in the U.S.

Please apply directly to by clicking 'Click Here to Apply' with your Word resume!

Looking forward to receiving your resume and going over the position in more detail with you.

- Not a fit for this position? Click the link at the bottom of this email to search all of our open positions.

Looking forward to receiving your resume!

CyberCoders

CyberCoders, Inc is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work - In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

Copyright &copy; 1999 - 2014. CyberCoders, Inc. All rights reserved.


